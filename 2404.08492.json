[
  {
    "type": "UncategorizedText",
    "element_id": "07cd30f6f89754a2c217419880a91514",
    "text": "4 2 0 2",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Header",
    "element_id": "aceae6233968ecadfec7bfce2956214a",
    "text": "r p A 2 1 ] N G . n o c e [ 1 v 2 9 4 8 0 . 4 0 4 2 : v i X r a",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "a77d31592b69ae8f1c1ec737341bb8b4",
    "text": "STRATEGIC INTERACTIONS BETWEEN LARGE LANGUAGE MODELS-BASED AGENTS IN BEAUTY CONTESTS",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "parent_id": "aceae6233968ecadfec7bfce2956214a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "1c24c4181548b0e5a034b3c26def66f8",
    "text": "A PREPRINT",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "parent_id": "a77d31592b69ae8f1c1ec737341bb8b4",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "419892c74cbb0b9c46045ee85ebef8ee",
    "text": "Siting Lu∗",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "parent_id": "aceae6233968ecadfec7bfce2956214a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "e5edb44f04940b0a46dbf3e42c32aae7",
    "text": "School of Economics, The University of Edinburgh",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "parent_id": "419892c74cbb0b9c46045ee85ebef8ee",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "95a56095ecab1eb5cd3133c2bc0c1da3",
    "text": "12 April 2024",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "parent_id": "aceae6233968ecadfec7bfce2956214a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "4a209a30f65a729a391a75f3320ec4af",
    "text": "ABSTRACT",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "parent_id": "aceae6233968ecadfec7bfce2956214a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "87c058dee9debd26be4a98ceaf5a150a",
    "text": "The growing adoption of large language models (LLMs) presents substantial potential for deeper understanding of human behaviours within game theory frameworks through simulations. Leveraging on the diverse pool of LLM types and addressing the gap in research on competitive games, this paper examines the strategic interactions among multiple types of LLM-based agents in a classical game of beauty contest. Drawing parallels to experiments involving human subjects, LLM-based agents are assessed similarly in terms of strategic levels. They demonstrate varying depth of reasoning that falls within a range of level-0 and 1, and demonstrate convergence in actions in repeated settings. Furthermore, I also explore how variation in group composition of agent types influence strategic behaviours, where I found higher proportion of fixed-strategy opponents enhances convergence for LLM-based agents, and having a mixed environment with agents of differing relative strategic levels accelerates convergence for all agents. There could also be higher average payoffs for the more intelligent agents, albeit at the expense of the less intelligent agents. These results not only provide insights into outcomes for simulated agents under specified scenarios, it also offer valuable implications for understanding strategic interactions between algorithms.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "parent_id": "4a209a30f65a729a391a75f3320ec4af",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "8aee581afe9e0a2611b248b92d31e626",
    "text": "Keywords: Large Language Models, Beauty Contests, Strategic Interactions",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "parent_id": "4a209a30f65a729a391a75f3320ec4af",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "3b318bbe7edc20edc1536bb1cfcd141f",
    "text": "∗This is a preprint version. I am immensely grateful to my co-authors of another paper, Guo et al. (2024), Shangmin Guo, Haoran Bu, Haochuan Wang, Yi Ren, Dianbo Sui, Yuming Shang, for providing some of the experimental results that enabled the progression of this line of research. I am also thankful to my supervisor, Ed Hopkins, and SGPE PhD conference discussant, Marco Pelliccia, for their invaluable suggestions. Author contact: s.lu@ed.ac.uk.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 1,
      "parent_id": "4a209a30f65a729a391a75f3320ec4af",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "8de2a6e07c3e4df67625a60f079764a4",
    "text": "Introduction",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 2,
      "parent_id": "aceae6233968ecadfec7bfce2956214a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "51d9fd6b25dc6a5f66e1a7ab5c09f8bb",
    "text": "With the emergent line of research surrounding the study of the capabilities of large language models (LLMs), there is also growing discussions over the implications of LLMs on economic research and the possibility of using them for social sciences experiments. While the current applications of LLMs as simulated agents mainly concentrated on more text-based tasks, such as filling in surveys as a way to provide insights on the average opinion of the population regarding certain topics, it could also impact more strategic scenarios, particularly in the field of game theory. Recent research in this area, however, mainly focused on exploring 2-player cooperative and non-cooperative games, and often comprises of a single LLM type. It could be interesting to investigate strategic interactions in multi-player competitive games, where agents can be back-boned by different LLMs to represent players of heterogeneous types. As a result, in this work, I seek to leverage on the presence of multiple LLMs to explore their strategic behaviour in beauty contest games.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 2,
      "parent_id": "8de2a6e07c3e4df67625a60f079764a4",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "9e4f5ccfdc6b6dc93484545c7d699a06",
    "text": "LLMs as simulated agents. One of the main objectives of this work is to make a case for the possibility of using LLMs as simulated agents, which still has limited applications in Economics. Since LLMs are trained based on human-generated data, observing game play for LLM-based agents could be fairly relatable to human subjects in experiments, thus their responses could provide results that offers more insights than the conventional simulation methods. Furthermore, it is relatively easy to use LLMs in experiments, making it a good tool to toy with different set-ups and to provide fresh insights on classical games. Even though studies did highlight LLMs’ ability to mimic human behaviour and reasoning process, there remain questions and controversies of using LLMs as replacement for human subjects. In view of the potential limitations in interpreting the minds of LLM-based agents with the absence of similar cognitive architecture that backs human behaviour, it is important to note that using LLM-based agents as participants of experiments is not aiming at replacing human subjects, but rather simulating potential game play by riding on top of embedded human behaviour. It is also more cost-effective if we want to investigate some variations that generate interesting insights from LLM-based games in a more expensive human-driven experiment, thereby offering designs that are worth pursuing.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 2,
      "parent_id": "8de2a6e07c3e4df67625a60f079764a4",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "bde4f321379aa42aa4e50e256d25f704",
    "text": "Behaviour of LLM-based Agents in Multi-player Competitive Game. The second objective of my work is to inform potential game play in a multi-player competitive game and study the adaptive learning behaviour of agents when the group composition differs. Literature that simulated agent behaviour with LLMs in games often consist of 2-player games and usually use a single LLM type (Horton (2023), Phelps and Russell (2023)). This is equivalent to assuming all agents have the same natural language understanding ability, modelling agent behaviour from which is therefore a little more restrictive and did not leverage on the potential of having a pool of LLMs. By having multiple types of LLMs, it is possible to draw relation of these models to human subjects. Using the beauty contest games, I found the models to have a strategic level between 0 and 1 using the level-k model proposed by Nagel (1995), and in repeated setting, some of them show convergence towards Nash equilibrium action, which could be at varying speed, as past information are revealed to them. Further to this, I have selected two different LLM types, characterized by their strategic levels, to explore how variation in proportion of agents types in the group could affect game outcome. In which case, I found less strategic agents could converge faster to Nash equilibrium than more strategic agents when facing fixed-strategy opponents. Furthermore, when they are playing against each other, their learning rate could be faster than when they are playing with their own types. Specifically, while less strategic agents do not learn when playing with own types, they do in mixed environment. However, in terms of payoffs, the more strategic agents gain comparably well or having better payoffs in mixed environment than pure environment, and this comes at the expense of less strategic agents. These contribute first to assessing the models with a human-based metric on the basis of strategic level, thereby drawing a relation to represent heterogeneous human subjects using the models , and then simulating game play and exploring learning behaviours. The variation in group composition also offers some insights on how to induce faster learning by changing proportion of opponent types faced by the agents.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 2,
      "parent_id": "8de2a6e07c3e4df67625a60f079764a4",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "01dbe8317c56ea96efe3523386c1f941",
    "text": "Strategic Interactions between AI Algorithms. Last but not least, this work hopes to use methods that evaluate human subjects to shine a light on how algorithms could interact with one another in strategic setting. For instance, with the ongoing progress of integrating LLMs into daily life, it is likely that in the future, agents will be using bots back-boned by LLMs to communicate and interact with one another, making LLMs surrogate agents. A clear example would be in the trading market, where existing crypto trading bots function by executing pre-defined strategies, making buying and selling automatic (Trality (2024)), it is entirely plausible to replace such automatic bots to ones backed by LLMs that take into account vast human data on trading strategies. Therefore, understanding how algorithms react to one another would be important in such context. Furthermore, since large LLMs could be costly, it is possible to have strategic situations that encompass a variety of LLMs, and one can trade-off between cost of adopting different LLM-backed agents and their performance.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 2,
      "parent_id": "8de2a6e07c3e4df67625a60f079764a4",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "6c040841948a26c14d1f3ee17ea852cf",
    "text": "On a broader perspective, this work hope to highlight that LLM-based agents not only can be used as a tool for social sciences in simulating potential human behaviour in strategic situations, the theories that were developed to evaluate human behaviours can nonetheless help us to understand how this new era of computer algorithms function when competing with one another.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 2,
      "parent_id": "8de2a6e07c3e4df67625a60f079764a4",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "06e086107b6cbfefb0513b563fd81604",
    "text": "The rest of the paper is divided into a few sections: Section 2 introduces the background of this work. Section 3 explores LLM-based agents in beauty contest games, divided into games with multi-LLM-based agents to investigate their respective strategic level, as well as convergence behaviour in repeated setting, and reducing LLM-based agents into two types, to investigate changes in their behaviour given variations in group composition. This is followed by Section 4, where I study the understanding and line of reasoning behind the choices made by the two LLM types for the different set-ups. Future updates and extensions are then highlighted in Section 5. Following which, I conclude in Section 6.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 2,
      "parent_id": "8de2a6e07c3e4df67625a60f079764a4",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "5749fdd6b67e4204b3047ba33540bc87",
    "text": "2",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 2,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "cc513115ba5c111516eaf55941d72b4c",
    "text": "2 Background",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "30d1ea276923aae57e859935657f508c",
    "text": "2.1 LLMs as Computational Model of Human Behaviour",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "59535851fe8c9591797537f8eb1e438a",
    "text": "While the creation of LLMs have sparkled a new wave of research in Computer Science to improve the models’ performance, their development also convey huge potential for interdisciplinary studies. Horton (2023) for instance proposed that LLMs can be perceived as implicit computational model of human behaviour and be used as simulation tool. Before diving into further details, it is important to understand how LLMs could function as computational model of human behaviour as a result of how they are trained.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "472af29017473bf3389047467560b587",
    "text": "Training Process. As illustrated by Ouyang et al. (2022), there is a 3-step training process that involves human feedback:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "dc74735b027122f0ab688c1d1570c33c",
    "text": "1. Training a supervised policy. A prompt is sampled from a dataset, and human labelers were asked to provide responses to the prompt, these response data are then used to facilitate supervised learning of the policy, constituting the early version of the language model.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "1cfa8fc881d0ac8347e0543c7a97fab4",
    "text": "2. Training a reward model. For a set of prompt and responses that were generated, again a human labeler gets involved to rank the responses from best to worst, this ranking data is then used to train the reward model.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "1996ed36ba97284b5d82f2d6150f5b8b",
    "text": "3. Optimizing the policy against the reward model. As the policy generates output and reward model computes the reward for the given output, the policy is fine-tuned against the reward model using reinforcement learning. This form of model alignment using reward model trained on response ranking from human annotators is referred to as reinforcement learning with human feedback (RLHF).",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "148b0198213d6af960a811a30a784d3c",
    "text": "There is implicit human involvement in the prompt sample, which rides on top of information that is publicly available on the internet, as well as licensed information from third parties, and there is also explicit human involvement in the training process that includes information generated by the LLM labelers and users. (OpenAI (2024)) Particularly, in the reward model training stage, the training dataset used by Anthropic primarily comes from crowd-sourcing feedback through Amazon Mechanical Turk, a platform often used for social sciences research. As for OpenAI, their models are trained on used prompts submitted by users to the GPT API. Since login are more restricted in this case, training based on feedback from different groups of users could induce performance differences across LLMs. (HuggingFace (2022))",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "8fac573527347b5ae0256985bddc30cf",
    "text": "Given that LLMs can be interpreted as computational model of human behaviour based on the underlying training process that pans on human-generated data, I hope to streamline and differentiate between the two main aspects of how LLMs’ human-like behaviour could cater to assisting and influencing research for Economics, or more broadly, the social sciences community:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "079f1295423f132ec980ec210899aeea",
    "text": "(a) Imitation of human decision-making process given known constraints. LLMs allow for the creation of synthetic agents with given profiles, and their behaviour, mimicking human behaviour under known and specified constraints, are investigated. This type of simulation could help to facilitate building of decision theory models when the constraints are pre-defined. It may resembles agent-based modelling (ABM) approach, where agents are endowed with pre-defined behavioural prompt, and the outcome from which could serve as a form of visualization and checkpoint of the theoretical predictions. However, as illustrated by Horton (2023), agents in ABM approach is completely pre-programmed to behave as we expect, whereas homo silicus, describing LLMs, are not completely under our control. While it is possible to provide them with artificial profiles and demanding them to behave with a pre-defined constraint, the outcome may not be exactly what we predict, and this could arise if there are implicit bias embedded in the underlying training data that the theoretical models, which was put to test, did not account for. Therefore, any deviation in the simulation results away from theoretical predictions inevitably makes it more interesting and useful, where they can feed back into improving theoretical models to explore potential factors that cause the diversion.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "863030263bd9fbcf102cca0dedd5659f",
    "text": "(b) Mirroring human-like complexities without known constraints. It is also possible to run simulation without pre-defining the constraints, and this would resemble running an experiment with human subjects, except with LLM-based agents. By abstracting away from putting restrictions on their behaviours, the simulation results would illustrate the less computational side of LLMs, and essentially offers a tool for computational experiments, showcasing the potential experiment results if they were conducted with human subjects. Depending on the baseline data the LLMs are trained upon, the results could be more representative of the average opinion of the general population or specific groups of population.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "fb254c1e1b0e863eee23cc4f45a0a359",
    "text": "While the first area spur emerging research in analyzing behaviours of artificial agents who were given pre-defined types or constraints, confirming and illustrating how certain known or hypothetical human bias could impact outcomes. The objective of which is more grounded in granulating the elements contributing to decision-making, which would be important in understanding why human behave the way they do. On the other hand, the second area takes advantage of the latent behavioural elements that were baked in the training data. Its main focus is to speculate the possible experimental results that can be anticipated if done with human subjects.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "f8c5d8e7aefe95a019f28527edc1c2df",
    "text": "LLMs as heterogeneous agents. LLMs are of different priors and could also comes in varying sizes, they can therefore be considered as heterogeneous agents on the basis of differences in these characteristics. The main difference between large-scaled language models and small-scaled language models are the amount of data they are trained upon. Since large-scaled language models are trained on massive amount of data, often comprising billions or trillions of words, they are potentially better at natural language understanding and providing better text-based outputs. (Labellerr (2024))",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "4e07408562bedb8b60ce05c1decfe3ad",
    "text": "3",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 3,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "dd047e8ef160c80681e496d20e894e1e",
    "text": "While we can directly perceive the models to be different types of simulated agents, their variation in text-based generating ability might not translate to varying strategic ability. Therefore, in this paper, their heterogeneity is characterized by their strategic levels that are determined when playing in the one-shot beauty contest games. This is a measure ubiquitous to how we evaluate the types of human subjects, thereby drawing parallels between the two. The additional benefit of this work leveraging on the existence of different LLM types to represent heterogeneous agents is that it also build a case where smaller language models can also be useful in helping us to understand human behaviour, rather than just learning from the larger language models.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 4,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "a17e6ac47c4d067591f4854f61a8cbf7",
    "text": "LLMs as complements to human participants. As the line of research emerge arguing for the usefulness of LLMs in social sciences, there is the question of whether LLMs can rise up to the task of participating in social experiments in place of human subjects or rational players. Firstly, current literature have explored if LLMs, mostly a single LLM type (often GPT3.5 or GPT4 ), could imitate human behaviours by attempting to replicate experimental results conducted with human subjects. Argyle et al. (2023) found GPT3.5 to reflect viewpoints that parallel to the US public opinions and sub-populations, and could behave as good proxies for aggregate level human cognition. There are also literature that illustrate LLMs’ strong capability in analogical reasoning, comprehension and communication skills to solve problems, as well as producing moral judgements that are well-aligned with human subjects. (Webb et al. (2023), Huijzer and Hill (2023), Dillion et al. (2023)) In the context of strategic games, LLMs were found to be able to reproduce the findings in many types of experiments, such as the ultimatum game, wisdom of crowds. AI bots equipped with GPT4 also exhibits behavioral and personality traits, in terms of risk-aversion and cooperation, as well as learning patterns, similar to that of many human subjects. (Guo (2023), Aher et al. (2023), Mei et al. (2024)) On top of modelling human behavioural features, there are also works that investigate how LLMs compare to rational players in games. Fan et al. (2023) indicates LLMs in games, such as dictator game, Rock-Paper-Scissors, and ring network, may find it hard to elicit uncommon preferences, refine belief, and may take sub-optimal actions because they have ignored or unnecessarily modified the refined belief. Guo et al. (2024) explores beauty contests and auction games and found LLMs to deviate away from Nash equilibrium action. While they may not serve the purpose of behaving as rational players in games, they inevitably shows ability to imitate human behaviours, as could serve as human-like simulated agents. Given their training process, responses from LLMs can be interpreted to ride on top of the human reasoning process, as well as their preferences and behavioural bias that are embedded into the training datasets. Therefore, it is likely that the eventual output from LLMs revealed the potential behaviour of human under each prompt, making LLM-based agent a plausible tool to approximate average human behaviour in experimental setting. Since training data rely less on strictly controlled environment and acquired from a potentially broader subject pool, the end results could be more representative of the population.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 4,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "372e6dc005b98901cee43fea13783bad",
    "text": "While Dillion et al. (2023) indicates LLM to be good proxy of a single participant, they question LLM’s ability to capture variability in human cognition as the model condense the diversity of responses into a modal opinion. This, however, can be solved by having different types of LLMs, which could be trained on different subsets of information, even for same training information, the degree of noise and processing ability also matters. Furthermore, in strategic situations, it remains interesting to investigate how game play within a homogeneous population is like.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 4,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "25ef42f38b3fb22cef39a66e047dc788",
    "text": "The main concern however, just like humans. While there are many theories that underline the behaviour of human in strategic situations, there is lack of something equivalent that strongly supports AI algorithms go through the same thinking process. However, since LLMs are trained using human-generated data, including reasoning (Kosinski (2023)) Despite this connection, it is procedures, they could develop mechanisms similar to that of human brain. important we treat the simulated results with caution. Therefore, my work does not aim at arguing for replacement of human subjects in experiments, but rather, in view of the parallels between strategic behaviours of LLMs and human subjects, we should take advantage of the tool to shed more insights on strategic behaviour of human subjects and their cognition process in specific setting. Further to this, instead of using it only when experiments are not feasible, it is a good and also cost-effective approach to gain fresh insights through variations in game set-ups when it is difficult to conduct the same experiment again, or simply as a way to find interesting experimental set-ups worth pursing with more expensive human subjects. This form of simulation could have implications for mechanism design, providing clarity on how variation in certain design could potentially impact game play.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 4,
      "parent_id": "30d1ea276923aae57e859935657f508c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "0245d819bdc3d06e1ed1f5cdcff9c7bf",
    "text": "2.2 LLM-based Agents in Competitive Games",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 4,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "463d3dfd8698d7206df0239ac45e5258",
    "text": "While some literature have explored cooperation and anti-coordination games between agents back-boned by a single LLM type, in this paper, I seek to explore competitive games that encompass multiple LLM types. As a result, I focused specifically on beauty contest games, which provides a classical and desirable set-up that encompasses both (1) competitive nature, (2) interactions between multiple agents. The agents involved can also be heterogeneous, capturing impact of differences in natural language understanding capacities or rather, intelligence or strategic levels for simulated agents. Furthermore, it is possible to construct a simple set-up with a single interior Nash equilibrium solution, making it easy to distinguish the levels of reasoning using either the iterated elimination of weakly dominated strategies, level-k or cognitive hierarchy models. (Nagel (1995), Camerer et al. (2004)) Further to that, Akata et al. (2023) studied repeated 2 × 2 games, such as Battle of Sexes and iterated Prisoner’s Dilemma, to inform coordination or anti-coordination behaviour of LLMs in repeated interactions, which has something to do with alignment with opponents. The same can be done for competitive games to explore LLMs’ adaptive learning behaviour when they are in direct competition with others and have to outmaneuver the others in order to win. Therefore, the additional benefit of repeated beauty contests under the standard set-up is that even in repeated setting, there is a single Nash Equilibrium, thereby setting aside any possibility of multiple equilibrium which could complicate the analysis.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 4,
      "parent_id": "0245d819bdc3d06e1ed1f5cdcff9c7bf",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "4b227777d4dd1fc61c6f884f48641d02",
    "text": "4",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 4,
      "parent_id": "0245d819bdc3d06e1ed1f5cdcff9c7bf",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "b02fdc0c6c6edff9b725c50b0228fb99",
    "text": "Nevertheless, there is also another layer of social value for exploring application of LLMs in the beauty contest games. The Keynesian Beauty Contest stems originally from Keynes (1936), and it has been used to describe the stock market, where investors attempt to guess what the other investors might do, and it is also related to beauty pageant competitions, where competitors were asked to select the prettiest contestant by forming expectation about other competitors’ opinions. (Nagel et al. (2017)) It is likely that in the future, trading bots or algorithms in competitive setting will be back-boned by LLMs, and potentially behave as proxies for humans to execute tasks. A more superior algorithm in terms of its parameter size or amount of training data it encompasses, does not necessarily imply better payoffs as they are playing in conjunction with other types of algorithms, and have to interpret and best react to what might the other algorithms do. Most importantly, individuals could choose proxies that are backed by different LLMs depending on their cost concern, making it important to look at how different types of LLMs could react to one another. Therefore, it becomes interesting to investigate how human-like LLMs interact with one another in a simple competitive setting like the beauty contests to inform more about its potential social implications.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 5,
      "parent_id": "0245d819bdc3d06e1ed1f5cdcff9c7bf",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "12117b4b3fdd9c5f0b856966042f6ad1",
    "text": "2.3 Strategic Interactions between Machines",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 5,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "231ba0749cdc0fcd36347dfb17bef5e4",
    "text": "Unlike previous generations of static computer algorithms that execute pre-programmed strategies, and behave regardless of the types of opponents they are faced with, LLMs are more human-like and dynamic in nature. In this work, as I simulate interactions between LLMs and static algorithm, as well as LLMs vs. LLMs, there can be many different interpretations. The results can be used to represent game play between synthetic human agents, human vs. computer, and computer vs. computer. In the first two instances, LLM-based agents are behaving as simulated agents that approximate human behaviour, providing some plausible conjectures about how human subjects might react in games when placed against other human subjects or against computer algorithms. While we use LLMs as simulated human agents to understand human behaviour, the reverse is true as well, through explaining the behaviour using concepts borrowed from human research, we can also understand machines better.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 5,
      "parent_id": "12117b4b3fdd9c5f0b856966042f6ad1",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "1cffe7d0e43db5082ead0f472bbe6247",
    "text": "For centuries, economists, psychologists and neuroscientists have been studying the quasi-black box of human brain and came up with numerous theories that seek to explain the decision-making of human, they can be used to potentially explain machine’s behaviour as well, or at least as a first step, drawing parallels between the two. For instance, Kosinski (2023) explores the application of theory of mind for LLMs and Dillion et al. (2023) also mentioned comparing LLMs with human judgements could perhaps teach us about the machine minds of LLMs. There are a lot more to be done for strategic interactions. By looking at the computational experiments and the human subjects’ experiments, we can evaluate the models using the method we use to evaluate humans, and we can perhaps use how we explain human interactions to explain machine interactions, thereby generating more insights on how minds of machine might work in strategic settings. Understanding how they function is a step forward to improving their performance in the future to be more human-like or even, exceeding average human capacities.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 5,
      "parent_id": "12117b4b3fdd9c5f0b856966042f6ad1",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Header",
    "element_id": "aec400e3e65dc09b31513694bc9893b9",
    "text": "5",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 5,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "ddf2e139ececb285ef5bdb1958b7051e",
    "text": "3 Beauty Contest Games",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "aec400e3e65dc09b31513694bc9893b9",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "3aef9f7c546cae81de8a447d58edccf2",
    "text": "The experiments in this section is based on the EconArena proposed by (Guo et al. (2024)). Different from Guo et al. (2024), where the main objective is to evaluate LLMs’ performance relative to rational players that play Nash equilibrium following game-theoretical predictions, this work aims to analyze LLMs’ behaviour as though they are human players, where their strategic levels are assessed using the methods employed to assess human subjects, and their potential game play is simulated under varied set-ups.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "ddf2e139ececb285ef5bdb1958b7051e",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "6cf779c6828abde4f902d85c978f1514",
    "text": "In this section, I first explore one-shot and repeated beauty contests games involving multiple LLM types, which include: ChatGLM2 , ChatGLM3 , Llama2 , Baichuan2 , Claude1 , Claude2 , PaLM , GPT3.5 , GPT4 After evaluating their strategic levels and convergence behaviours, I then choose two types of LLMs, namely PaLM and GPT3.5 , which were assessed to be of different strategic levels, to construct groups of heterogeneous agents, and analyze how variations in group composition could affect game outcomes, providing some insights on their adaptive learning pattern.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "ddf2e139ececb285ef5bdb1958b7051e",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "da8e3572b9f7757b161612c3a9e50740",
    "text": "3.1 Multi-LLM-based Agents",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "aec400e3e65dc09b31513694bc9893b9",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "70dbb7e724975cffd1b880290aaab6c1",
    "text": "The basic set-up is a modified version following Nagel (1995) and the original exemplary prompt is recited in A.1.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "da8e3572b9f7757b161612c3a9e50740",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "bcddbde85c479951bbf00607da1831a7",
    "text": "The agents are asked to choose a number between 0 and ¯c, where ¯c ∈ (0, 1000). • The winner is the person whose number is the closest to p of the average of all chosen numbers, where p = 2 3 . • A fixed prize of $x will be awarded to the winner, and in case of a tie the prize is split amongst those who tie. • The same game can be repeated several periods, in each period, subjects are informed of the mean, 2",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "da8e3572b9f7757b161612c3a9e50740",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "b6c7545367f6bb9379a2e93dec845a51",
    "text": "choices. 3 of the mean and all",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "da8e3572b9f7757b161612c3a9e50740",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "9bb1dba5fb5821759f2bf7a39d6e2a8a",
    "text": "• Theoretical prediction for this game points to a unique interior Nash equilibrium solution of 0.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "da8e3572b9f7757b161612c3a9e50740",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "b89279271c48ce978cfa5ea06e0a6645",
    "text": "I will focus my analysis in the following areas:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "da8e3572b9f7757b161612c3a9e50740",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "c34c03e415d02f939212141a25409618",
    "text": "1. Strategic Level. The frequency of choices will be explored and analyzed. Following Nagel (1995), an agent is of strategic 3 )n, where r is defined to be the reference point. This could be a choice characterized degree n if he chooses a number r( 2 by naive player or a point of salience in heuristics, which I will define in later parts.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "da8e3572b9f7757b161612c3a9e50740",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "55d31eeba2c8d72e235e4ac14ce7eb60",
    "text": "In the repeated game setting, every agent is responding to the information revealed from previous rounds. The changes in choices adopted by the agents can be tracked to determine if there is convergence to the unique Nash equilibrium of the game.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "da8e3572b9f7757b161612c3a9e50740",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "9c50961861d703ca4efed93f98f38016",
    "text": "3. Frequency and Evolution of Strategic Level. The frequency of strategic levels across periods was also tracked to get a gauge of the average strategic levels of the simulated agents that does not simply dependent on the choices made in the first round. It is also possible to explore if strategic level do evolve over time. The evolution of strategic level can be found by adjusting the reference point of each period, fixing it to be the mean of the previous period. If n increases, it can be interpreted as increasing level of thinking or revision in actions to best respond to opponents that could be of higher strategic level.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "da8e3572b9f7757b161612c3a9e50740",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "ea473dbad3da7a71ba14a7fada238e2c",
    "text": "4. Evolution of Payoffs. The most interesting portion about multi-agent competitive games is the possibility that stronger models might not always be better in generating the highest payoffs. Therefore, exploration of payoffs and their transition over time could be important to determine how agents’ performance could be dependent on their own strength as well as on the environment.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "da8e3572b9f7757b161612c3a9e50740",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "d90238949a42057f5eeb7d6572a97975",
    "text": "3.1.1 One-Shot Game",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "aec400e3e65dc09b31513694bc9893b9",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "d7b5deabcd01f6a48049215b969e4deb",
    "text": "150 sessions of one-shot beauty contests were ran with 9 LLM-based agents back-boned by multiple LLM types. These games are used to determine the strategic level of each model.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "d90238949a42057f5eeb7d6572a97975",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "3d31a0a2b7c307e15ee99a65981d3ca1",
    "text": "In the standard beauty contests, where ¯c is fixed to be 100. Via the iterative elimination of weakly dominated strategies, all choices between (66.66, 100] are weakly dominated by 66.66, and all choices above 44.44 are weakly dominated by 44.44, etc. Going by the level-k model, for a focal point set at 50, level-0 chooses 50, and level-1 responds by choosing 33.33, etc. In this modified set-up, a randomly generated upper-bound, ¯c, is used for each one-shot game. The upper-bound will not affect the assessment of strategic level. For instance, using the level-k model, level-0 would choose the focal point, which would be the ¯c mean of uniform random choices, ¯c 2 , so on and so forth. The average strategic levels of each model are computed in this manner to ensure a more robust measure of strategic level that is consistent across changing game parameters.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "d90238949a42057f5eeb7d6572a97975",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "9dadedc8bae20601e2aa4a8bd9384ac0",
    "text": "Choices. The frequency of normalized choices made across the 150 sessions of one-shot games is shown in Figure 1. The choices are concentrated on 50 for ChatGLM3 , Baichuan2 , Claude1 , PaLM . Based on the level-k model proposed in (Nagel (1995), these models can be characterized as naive or level-0 players. The average choice for Llama2 lay around 60, approximately corresponding to level-0, and the choices are noticed to be fairly dispersed across the range of 40 to 80. Claude2 ,",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "d90238949a42057f5eeb7d6572a97975",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "e7f6c011776e8db7cd330b54174fd76f",
    "text": "6",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 6,
      "parent_id": "d90238949a42057f5eeb7d6572a97975",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "0d4a00318af193cebf998df12715b124",
    "text": "GPT3.5and GPT4 , on the other hand, are displaying slightly different choice frequencies as compared to the previous models. For Claude2 , there is a spike around 33, indicating the likelihood of level-1 thinking, on top of that, there is high choice frequency around 66 as well, which can be rationalized via step 1 of the process of iterated elimination of dominated strategies. (Mauersberger and Nagel (2018)) As for GPT3.5, most of the choices are also concentrated around 33, stipulating level-1 reasoning. While there are some other spikes at 50 and 66, but are of much lower frequency. Lastly, for GPT4 , the highest spike in choices is around 44, going by iterated elimination of dominated strategies, this could imply step-2 of the depth of reasoning process, a lower frequency spike is also seen at around 33, indicating level-1 thinking via the level-k model. There are no data for ChatGLM2 as it is unable to complete any of the games, which can be attributed to it being a relatively weak model and thus is unable to produce any output based on the instructions. Based on the choice frequency, it is not surprising that the larger language models are able to generate higher frequency in choices that can be characterized as higher level of strategic thinking. One further detail to notice is that for Claude2 and GPT3.5 , there are instances where they in fact select the NE choice of 0, consisting 6% and 5.3% of the sessions respectively.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 7,
      "parent_id": "d90238949a42057f5eeb7d6572a97975",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "4c404076e70c4c997aa0ddfd2553608d",
    "text": "Histogram of choices for ChatGLM3 o 20 ) 60 80 100 Choices Histogram of choices for ChatGLM2 oo 02 04 o6 o8 1o hoices Histogram of choices for Llama2 Fequency = ) 20 0 ) 50 ) o 50 Choices Histogram of choices for Baichuan2 o ' a 10 B B W0 50 0 £ B Cnoices Histogram of choices for Claude2 Frequency & r— - o 10 20 B 2] 50 60 i hoices Histogram of choices for Claudel F% F20 £ P R —— 20 EY W 50 B o B % 100 choices Histogram of choices for PaLM 60 §a0 £ o B a0 as By s 0 Cnoices. Histogram of choices for GPT3.5 p40 320 : ] | ol 1 o 10 20 By W 50 &0 o Cnoices Histogram of choices for GPT4 7 £ 50 2 J o",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 7,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "918f87fafb07bde880c6f63da77c7d68",
    "text": "Figure 1: Frequency of choices across 150 sessions for each model.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 7,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "7902699be42c8a8e46fbbb4501726517",
    "text": "7",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 7,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "88793ae262c9f39bdf393f1cedf9a9d1",
    "text": "In Nagel (1995) and Bosch-Domenech et al. (2002)’s period 1 results done on different human populations, including students (mean choice=36.73, median choice=33), theorist (mean choice=17.15, median choice=15∗), newspaper readers (mean choice=23.08, median choice=22∗), etc.2 The human subjects experiments shown strong deviation away from game-theoretic solution, and going by the level-k model, they on average display iteration steps 1 and 2. Even though it is often expected that the general public is more noisy in choices, in this case, newspaper readers have slightly lower mean than the student population, possibly due to longer time for reflection and thereby making more contemplated decisions. (Mauersberger and Nagel (2018)) Evaluating the simulation results of one-shot beauty contests with LLM-based agents against the results with respect to human subjects, they seem to be choosing slightly higher numbers that imply a lower average strategic level of 0 to 1 as compared to human subjects. However, this result does comply with the impression that general public should display more noise or randomize their choices more, inducing a lower strategic level on average. Even though it can be argued that LLM-based agents are all representing the general public, there remain heterogeneity among them. Given that the alignment of models are riding on top of data generated with different sets of labelers and users, the simulated agents back-boned by different LLMs can represent different subsets of the general populations, the difference is that types are not defined based on one’s career for instance, but characterized by their revealed average choices and strategic levels. As a result, games with multi-LLM-based agents describe a predicted outcome for games played by the general population, but involving individuals with heterogeneous degree of strategic thinking.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Table",
    "element_id": "af7f10d0b643fe5af68bd6ff501fedda",
    "text": "ChatGLM3 52.029 51.724 ChatGLM2 N/A N/A Llama2 59.519 62.685 Baichuan2 51.158 50.0 Claude2 41.609 33.333 Claude1 47.696 49.313 PaLM 49.976 50.0 GPT3.5 38.912 33.333 GPT4 41.072 44.442",
    "metadata": {
      "text_as_html": "<table><thead><th>Models</th><th>ChatGLM3</th><th>| ChatGLM2</th><th>| Llama2</th><th>| Baichuan2?</th><th>|</th><th>Claude2</th><th>| Claudel</th><th>| PaLM</th><th>GPT3.5</th><th>[ GPT4</th></thead><tr><td>Average</td><td>| 52.029</td><td></td><td>59.519</td><td>S51.158</td><td></td><td>41.609</td><td>47.696</td><td>49.976</td><td>| 38912</td><td>41.072</td></tr><tr><td>Median</td><td>| 51.724</td><td></td><td>62.685</td><td>50.0</td><td></td><td>33.333</td><td>49.313</td><td>50.0</td><td>33.333</td><td>44.442</td></tr></table>",
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "532b497b424a18015581dd4b8906b3c7",
    "text": "Models Average Median",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "123f4ccb423e2ebfaba8e9a72138667a",
    "text": "Table 1: Average and Median Choice of the LLMs across 150 Sessions",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "66f99263ea93c99ab154760c0a1745c2",
    "text": "A possible follow-up question is that if the models are given the exact same instructions, would they choose the same number consistently? For human subjects, when given identical game set-up, it is possible that they might employ different strategies. There could be many reason behind this. Devetag et al. (2016) highlights the potential of players focusing on different features of the games across the series of one-shot games, thereby selecting different strategies for identical games. Further, Costa-Gomes and Weizs¨acker (2008) indicated the discrepancy between individuals’ stated beliefs and actions taken, implying one’s actual choices might be affected by factors not aligned with this understanding, which translate into inconsistency in strategies. The same could apply to LLM-based agents. If each LLM-based agent were to represent a single type of human agent, it could be important to understand how varied one’s choice of number might be given the same instructions. This also applies if each LLM-based agent were to represent a single type of population, where variation in choices could be further explained by proportions of agents playing different pure strategies, which might lead to different revealed actions in each round of identical games. This exercise is important to determine if, like human players, there could be variability in choices.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "parent_id": "123f4ccb423e2ebfaba8e9a72138667a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "15bd0c04cdf52c1f28741f2d092010de",
    "text": "When using LLMs as simulation tool, it is possible to some extent to control for the variability in the strategies executed by agents through changing the temperature. Chollet (2021) introduced softmax temperature in the text generation process, which characterizes the randomness of the choice of the next word. Higher temperature corresponds to less predictability of the output, and lower temperature indicates more deterministic result. However, though this feature characterized the randomness in text output, its impact on strategic choices is not as clear. Therefore, by exploring the revealed choices given the same level of temperature is being set for all the models, this can in fact determine how random the models are in strategy selection while holding all else constant.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "parent_id": "123f4ccb423e2ebfaba8e9a72138667a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "7a978df8685e1631f4e8fe121c064c4d",
    "text": "Difference in Chosen Number For the Same Upperbound Average Values ° | | | | ChatGLM3ChatGLM2 Liama2 Baichuan? Caude2 Caudel PalM GPI35 GPT4 Model",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "d04eea61af0a6f186fc50a3012ff46b4",
    "text": "Figure 2: Variability in chosen number given the same upper-bound.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "c564345c96d790967d24dd5121c0f04b",
    "text": "2The median for the later groups were not computed, but it is guesstimated based on the figures in Nagel (1995) and Bosch-Domenech et al. (2002) to be around 15 for theorist and 22 for newspaper readers.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "2c624232cdd221771294dfbb310aca00",
    "text": "8",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 8,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "bc36ae2b9154ff71c4dd5b2dc11d1d6c",
    "text": "Figure 2 shows that within the 150 sessions, for those sessions having the same upper-bound of choice range, the same LLM-based agent could choose slightly different numbers. For instance, Claude2 , GPT3.5 and GPT4 displayed more variability in choices as compared to other models. This results is indicative that choices might not be static even when the instructions is exactly the same, which further emphasize the need to run many sessions of one-shot games that encompass both identical upper-bounds and different upper-bounds, the determination of average choices and the corresponding strategic levels based on which would render a more consistent and robust measure for each model.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "067344c4aa4482fb108b14e1cf816c58",
    "text": "Strategic level relative to reference point. To evaluate the exact strategic level of each model given their choices, I follow (Nagel (1995))’s method used for human subjects. One’s strategic level is denoted as n, and r is the reference point, the 3 )n = chosen number. In Mauersberger and Nagel (2018), the reference point computation of strategic level is determined using r( 2 refers to the choice of a non-strategic agent, who is assumed to play the mean of the range of choices, pertaining to insufficient reasoning. However, this focal point can be arguable. In these current set of beauty contest games, the upper-bound varies, therefore, calculating for the mean of uniform random choices may not be as straightforward as games with fixed upper-bound, thus it could be reasonable for the upper-bounds to be the focal points as well. Figure 3 shows that for the 150 sessions, the average strategic level of each LLM-based agent lay between 0 and 1 when the reference point is set to mean of random choices (i.e r = ¯c 2 ) and between 1 to 2.5 when the reference point is set to be the randomly generated upper-bound of each game (i.e. r = ¯c).",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "f930f81a6f9ee1f727770238bfc4cec0",
    "text": "For the conventional focal point of ¯c 2 , Figure 3a shows that the average strategic level for Claude2 and GPT3.5 are comparable. Surprisingly, GPT4 has slightly lower strategic level relative to these two models, even though it is often assumed to be a stronger model than GPT3.5 by being faster and more accurate, but this result shows that it is not necessarily true that GPT4 is of a higher strategic level. In relation to human subjects, a model with higher strategic level could represent agents with higher intelligence and/or believing that their opponents are of relatively high strategic level, thus this difference between GPT4 and GPT3.5 could result from either or both of the preceding reasons. Since GPT4 is known to be trained on more data, which could result in more noise in strategies employed, it could be argued that agents back-boned by GPT4 is likely to have an initial “belief” that its opponents have relatively low strategic level, thus leading to guesses that corresponds to lower depth of reasoning. Further, while the average strategic level of ChatGLM3 is comparable to that of GPT4 , its variability is much higher. This could suggest that smaller models can representing agents who either have lower strategic level or larger variability in depth of reasoning, expanding the dimensions of types of agents the models can simulate for.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "272c4e098703612266dc118544096e29",
    "text": "Meag ?trategic Level and Standard Deviation for Each Model (150 Sessions) .. - 20 15 10 < o5 . ﬁ 0.0 - + —+ 05 -1.0 ~ GRS CaGE e Gz Omon Ows W TS Gne Models",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "f4c689c61c8d24bdc7afc67c19d69ecd",
    "text": "Mean Strategic Level and Standard Deviation for Fach Model (150 Sessions) 4.0 35 3.0 25 <20 15 10 0.5 0.0+ AN CAGLGE Lmaz Bchumz G Gmal R GRS @R Models",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "e2f24a15857ddccad2495849828fdf43",
    "text": "(a) r = ¯c 2  (b) r = ¯c ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "df5c448dce7940f6d904433ae21a6d0e",
    "text": "Figure 3: Average strategic level of LLM-based agents in beauty contest games with difference reference points.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "3e8d0a5b5df8deac28d075b1a6f27c74",
    "text": "Payoff. Apart from choices and strategic levels, it is also important to learn the average payoffs earned by each model. In Figure 4, it can be seen that Claude2 , GPT3.5 and GPT4 have relatively higher average payoffs than the others. Among the three, standard deviations are comparable for Claude2 and GPT3.5 , and GPT4 has a slightly lower standard deviation than the other two. Despite GPT3.5 being the best in average payoffs as compared to the other models, its variability in payoffs is also relatively high, making large and small gain equally likely. Coupled with the average strategic level of each model, it can be seen that the ones with higher strategic levels often obtain higher payoffs, except for ChatGLM3 , whereby the variability in its strategic levels could have adversely influenced its average gain.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "c25d115ad22b545a32ead2fd22bdce15",
    "text": "Overall, I have analyzed the average choices of each LLM-based agent in the one-shot beauty contest games, and their strategic levels were determined using methods that were previously employed to evaluate human subjects in experiments. In a way, this facilitates better understanding of machine behaviour by drawing on theories that were used to study human decision-making. Herein, the models also showcased their potential in representing heterogeneous agents, who have different",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "19581e27de7ced00ff1ce50b2047e7a5",
    "text": "9",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 9,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "65d5944bf206c87530fc779a74c71feb",
    "text": "strategic levels. Through varying the model selection, degree of heterogeneity in the population to be simulated for can be controlled to some extent.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "06119ca7d5eee8920c97375dbbde41cb",
    "text": "Average Payoffs and Standard Deviations for Each Model (150 Sessions) 08 0.6 0.4 Payoffs 02 00 | - -— - CuGLN3 OWGLR Usma2 Bichan? Caude2 Caudel RN @3 @ Models",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "79849203ede68b93914bf91f2c367b57",
    "text": "Figure 4: Average payoffs across 150 sessions for each model.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "0f4dffd23529d5b7f8ccbc3fbb7144a3",
    "text": "3.1.2 Repeated Games",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "65dbc5a62fa203b8d30ca2185ee84e3b",
    "text": "In this subsection, I continue with experimental results from Guo (2023), but under the repeated setting of 6 periods. The prompt to include historical information is detailed in Appendix A.1. Across the periods, the game parameters faced by the agents do not change, thus they choose numbers within the same range of [0, ¯c], and ¯c is randomly generated in period 1. Historical information, including choices made by all agents, average of these choices, 2 3 of the average, and the winners from the previous round, are revealed up to 3 periods. The limitation on revealing 3 past periods is as a result of token restrictions to control the computational intensity. Such setting can be seen as partial feedback or feedback with a forgetting parameter. It is expected that agents will react to the information and slowly converge to the Nash equilibrium.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "parent_id": "0f4dffd23529d5b7f8ccbc3fbb7144a3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "6466384447cec9c98945e04c9bcfe7a7",
    "text": "The repeated beauty contests ran for 30 sessions. As before, via period 1 choices, the corresponding average strategic level for each model, assuming the reference point to be ¯c 2 is determined to be between 0 and 1, shown in Figure 5a. In Figure 5b, I explore the average chosen number across the 6 periods. The numbers are normalized for better comparison across different sessions that could have different ¯c. It is shown that with historical information, most LLM-based agents converge in actions, particularly for Claude1 , Claude2 , GPT3.5 , and GPT4 , their average normalized chosen numbers are approximately 0 in period 6. Suppose models with higher strategic level represent more intelligent agents, then the “smarter” agents do display “learning” pattern given historical information, where they converge to the unique Nash equilibrium of 0 across time.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "parent_id": "0f4dffd23529d5b7f8ccbc3fbb7144a3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "5331191d87a0c4197c75ef03c0b82b53",
    "text": "Period 1 Average n and Standard Deviation Across 30 Sessions for all LLMs 25 T 20 15 10 < 0.5 I BN . iy -0.5 1.0 1 (ChatGLM3 ChatGLM2 Usma2 Bsichuan? Caude? Caudel PalM @ns @n Models",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "a7136ad7f82e2cec8865603f86007532",
    "text": "Average Normalized Chosen Number Across 30 Sessions For all Models 10 —#— ChatGLM3 —#— ChatGLM2 5 —=— Uama2 € o8 —=— Baichuan2 El —#— Claude2 < = Claudel 2 o6 ~#— PalM = —#— GPT3.5 b —#- GPT4 E 04 2 : & £ 0z 2 0.0 1 2 3 4 5 6 Period Index",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "b9a565c3bbf661ab0ccac232d2bd1f01",
    "text": "(a) Period 1 strategic level.  (b) Average chosen number. ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "fec273f47dc2e1c3e80a87a24471d085",
    "text": "Figure 5: Strategic level determined by period 1 choice and convergence in average chosen number over 6 periods.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "2315852aca8bcf4e3227f45a63503406",
    "text": "10",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 10,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "758be17d737091047aa8c221638d5c30",
    "text": "Frequency and Evolution of Choices and Strategic Levels: Given the repeated setting, it is possible to track for changes over time, as a result, the frequency of choices and the corresponding strategic levels are computed over the 6 periods and across 30 sessions in Figure 6.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 11,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "189b1c5fe59f17e29edf4fc2a056d4ba",
    "text": "Histogram of choices for ChatGLM3 515 g0 i ol PR o 2 ) © 0 ) B3 Cholces Histogram of choices for ChatGLM2 aos £ oo H 005 oo 0 ] 3 as 0 Choices Histogram of choices for Liama2 2 g S o S0 s w0 w5 mo ss ee 65 Choces Histogram of choices for Baichuan2 >0 H i H ol PO o B » 23 0 w0 n choices Histogram of choices for Claude2 Z 0 g HE J ol pal od ) 20 E) w© 50 ) Choices Histogram of choices for Claudel §° g & . as py 20 » © 0 s Choices Histogram of choices for PaLM =20 H g H o o B » W 50 ) choices Histogram of choices for GPT3.5 15 g0 £ 1 ol - o 20 » w E) 2y Choices Histogram of choices for GPT4 520 § 2 S0 RN EPPIFITY ¥ T Y S Y W ol b 5 2 2 0 E3 ) & Choices",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 11,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "bd522edd97e13832373d4326bda191f6",
    "text": "Histogram of n values for ChatGLM3 (30 Sessions) . > g* i l N | Il [ | 1 5 T B 3 ) 3 3 nvalues Histogram of n values for ChatGLM2 (30 Sessions) 005 £ o] ————m———— e £ 005 P 02 os a6 os 1o nvalues Histogram of n values for Llama2 (30 Sessions) 3 22 2 0 %0 oo o0 050 o%e ot otz nvalues Histogram of n values for Baichuan2 (30 Sessions) . > g1 H &2 ‘ ‘ o | el |1l | 1 00 2 as oo os 10 2 Ta nvalues Histogram of n values for Claude2 (30 Sessions) . > §* g> Lk . Lyl oo o5 1o s ) nvalues Histogram of n values for Claudel (30 Sessions) . 2, g &2 . 1 | o0 o5 10 s ) nvalues Histogram of n values for PaLM (30 Sessions) . > 8¢ 3, H o o0 o 0% obs 1o 13 1% 11 nvalues Histogram of n values for GPT3.5 (30 Sessions) € g = ful u . L1 gule w1 w1 0 os 10 ps i) nvalues Histogram of n values for GPT4 (30 Sessions) . > 24 g g 2 Il | L m 111 | L il 025 050 o7 100 125 1% s 200 nvalues",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 11,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "efb212335811f6a815e1dfa51573b1de",
    "text": "(a) Choices.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 11,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "4f9297da1e4108b444c4188f7b3b617c",
    "text": "(b) Strategic levels.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 11,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "7e0054aa1029bd9c3652074a4801e7dc",
    "text": "Figure 6: Frequency of chosen number and strategic level calibrated to new reference points over 6 periods across 30 sessions.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 11,
      "parent_id": "4f9297da1e4108b444c4188f7b3b617c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "0452566bf65b8072b8e532e97f62b071",
    "text": "In Figure 6a, for most of the LLM-based agents, the choice of 50 receives the highest frequency, but Claude1 , GPT3.5 and GPT4 do display more selection below 50. Since the choice frequency consists of period 1 to 6, the histograms display greater dispersion in number selected as compared to the one-shot games in Figure 1, which is an indication that agents do change their",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 11,
      "parent_id": "4f9297da1e4108b444c4188f7b3b617c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "c8a93e46e1a3cb053ea8185a8d7a873c",
    "text": "11",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 11,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "5d23c74c1293044f1edf815c38adeabb",
    "text": "actions over time, and there is more selection of numbers lower than 50. Following Nagel (1995) method of computing for strategic levels in repeated setting, where the first period reference point is fixed at ¯c 2 , and in subsequent periods, the reference point is re-calibrated to be the mean choice of the previous period. In experiments with human subjects, the results show no support for increasing depth of reasoning, most subjects remain in the bounds of iteration step 0 and 3, and they rarely go over iteration step 2. With LLM-based agents, based on the frequency of strategic levels computed across periods in Figure 6b, the results are similar, they do not go over iteration step 2, and most of them display spike at 0 and 1, with GPT4 showing spike at both 1 and 2. However, even though they are comparable to human subjects about staying below iteration step 2, some of them do display minor increases in strategic level as compared to their period 1 performance. In particular, GPT4 has strategic level much lower than 2 in period 1, but is able to achieve iteration step of approximately 2 with high probability over time.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 12,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "ccccc17013749c18583ea9b280dc3ff7",
    "text": "Average n acrass sessions for each period chataLm3 chataLm2 Uama Baichuan Claudez Claudel FaLM GPT3.5 GPT4 08 IRELXREEY 06 Average n 04 02 00 period",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 12,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "4363618df4bab46e7674cae3284d52fc",
    "text": "Figure 7: Averaging strategic level across 30 sessions for 6 periods.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 12,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "0c82a0dceeb8a6dcd4bd5b5be116a275",
    "text": "For illustration, I explore the changes in strategic level for each model, averaged across sessions, for each period in Figure 7. It shows that strategic levels evolve over time, but the range of that is rather narrow, on average they remain within the bound of 0 and 1.4. Most LLM-based agents shows increasing depth of reasoning as defined by the growing strategic level, this is especially true for stronger models, Claude2 , GPT3.5 and GPT4 , where strength is characterized by the relative strategic level computed in period 1. The abnormality comes from ChatGLM3 and Llama2 . Llama2 has a relatively low strategic level in period 1 and did not display improvement over time, its lack of change could imply some kind of non-strategic or naive behaviour, where they keep behaving randomly. As for ChatGLM3 , the LLM-based agent do have relatively higher strategic level than the other agents in period 1, but yet, its average strategic level across time decreases. This could imply its lack of ability to respond to historical information and adjust its behaviour accordingly.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 12,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "00620ff80ad9617f4f8a6ac28d708291",
    "text": "Evolution of Payoffs: Figure 8 shows the payoffs for each LLM-based agent over time, averaged across 30 sessions. GPT3.5 outperforms the rest in all periods, while Claude2 and GPT4 are more or less comparable. The rest of the LLM-based agents do not obtain average payoffs as high, but most of them display growth over time. Coupled with Figure 5b that shows convergence in average choice towards NE of 0 for most LLM-based agents, the increasing payoffs could be an indication of learning about the optimal action to take to win the game and also obtain higher payoffs. The exception is Llama2 , it does not finish the 6 periods and average actions display no convergence, thus its corresponding average payoffs is also consistently 0 throughout.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 12,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "df2873be1a5f66b93b2b64aa299948dd",
    "text": "Transition of Average Payoff Across Periods —=— ChatoLms o5 —= ChalGLmM2 —= Uamaz . —= Baichuanz 04 T —=- Caudez T e—— \"= | - cauwel —= paLM 03 = GPT3S = GPT4 Average Payoff 9 01 0o period",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 12,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "cb40c6593076ba88e24cfd336c5e18ea",
    "text": "Figure 8: Average Payoffs across 30 sessions over 6 periods.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 12,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "a50eab9333a59d648039f387b1ab3d3d",
    "text": "12",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 12,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "1ed098765cc8d049be66f8d81df6e082",
    "text": "In this section, I explored the outcomes of heterogeneous population comprises of multi-LLM-based agents to illuminate strategic interactions between heterogeneous agents or algorithms if LLMs were adopted as proxies.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "8e9f1a857e073149c0a90889067b4faa",
    "text": "The purpose of one-shot games, as well as the first period of the repeated games, are used to evaluate the strategic level of different LLM-based agents, with the objective of exploring their behaviour as simulated agents that we can draw parallel to human subjects. I found that models have varying strategic levels, the ones with larger parameter size do not necessarily have higher relative strategic level, but models with higher strategic levels tend to achieve better payoffs on average. The results from one-shot games resembles to that of experiments with human subjects, where agents tend to display low levels of reasoning, however, the distinction is that the average iteration steps are slightly lower for LLM-based agents than that of human subjects in experimental setting. Since LLMs encompass data trained on potentially larger and more general groups of population, and experiments with human subjects tend to explore more specific groups of population depending on the research needs, the degree of noise in strategies employed and the beliefs about opponents’ depth of reasoning could be different, thereby leading to slight distinction between game results ran with LLM-based agents and human subjects. By evaluating machine behaviours using traditional analysis methods conducted on human subjects and compare the results between the two, this helps not only to understand machine behaviour in human-like terms, it also draws some insights on what type of agents the LLMs can represent in game simulations.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "a3ba8c5de1d4a73210c30082db3b38bb",
    "text": "The LLMs involved in the games could vary in parameter size, which is a measure for complexity of neural network structure that is potentially in equivalence to human brain capacity (i.e. memories and ability to represent complex pattern), it seems straightforward to define one dimension of heterogeneity in LLM-based agents based on degree of complexity, however, even though LLMs can mimic human performance in complex tasks, their similarity to brain anatomy and physiology is imperfect, connecting LLM-based agents and human subjects on this line of differentiation could be problematic. (Pulverm¨uller et al. (2021)) Therefore, instead of distinguishing LLM-based agents on the basis of their resemblance to human brain capacity, it is more precise to characterize their heterogeneity in terms of potential difference in their underlying training data, revealed strategic levels and variance in choices in games, such as the one presented in this paper.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "acf23a7707746ef2f6ae9467c183e29c",
    "text": "Apart from the one-shot beauty contest games, I have also explore the repeated setting, which shine a light on how the simulated agents could behave over time, if they respond to historical information and whether there is any improvements in their depth of reasoning. Similar to human subjects, while LLM-based agents do not display iteration steps that go over 2 for the span of the games, they do seem to learn from historical information and show convergence in actions towards NE.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "d4120db9a5a0702ec5669aa0ddd8a851",
    "text": "3.2 Adaptive Learning with Variation in Group Composition",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "aa50e4d19027602ec3c0ab2f7a5cf192",
    "text": "To further analyze the strategic interactions between LLM-based agents, one possible aspect would be to investigate their adaptive learning behaviour given variation in group composition. Since I have already identified the LLM types in terms of their strategic levels, it is possible to conduct a simulation between heterogeneous agents, and observe their dynamics of game play by varying the proportion types for each session. Such repeated interactions in competitive games could assist in understanding what would human subjects do in similar set-ups, knowledge of which could be helpful in informing policy design. Furthermore, it would also foster better understanding of how algorithms will react to one another if they were adopted widely in competitive setting.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "parent_id": "d4120db9a5a0702ec5669aa0ddd8a851",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "bd6fc87038b7ceb42978d6f9f33bf211",
    "text": "In the following sections, I have selected two LLM types, a stronger and a weaker model, GPT3.5 and PaLM, where the strength is characterized by their strategic levels as analyzed in the previous section. They can be used to represent simulated agents who are more “intelligent” and “less intelligent”, or simply algorithms that are stronger and weaker. The games follow a revised design from Nagel (1995):",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "parent_id": "d4120db9a5a0702ec5669aa0ddd8a851",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "cb9773c3daa365b355c8f2d60b8507d7",
    "text": "• 10 LLMs-based agents are playing in each game.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "parent_id": "d4120db9a5a0702ec5669aa0ddd8a851",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "f8d9f557cf3101d316068b7c9bc0915f",
    "text": "The same group plays for 5 periods, and all history are revealed. • They choose a number between 0 and ¯c, where in this section, ¯c is fixed to be 100. The winner is the agent whose number",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "parent_id": "d4120db9a5a0702ec5669aa0ddd8a851",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "fadfd338553a69e49f22034b179de599",
    "text": "is the closest to p times the average of all chosen numbers, where p = 2 3 to ensure a unique interior NE solution.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "parent_id": "d4120db9a5a0702ec5669aa0ddd8a851",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "8ae49dc931e92209e44361fd3837a33c",
    "text": "• In each period, the winner gets a fixed prize of $x. In case of a tie, the prize is split amongst those who tie. All other players receive 0.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "parent_id": "d4120db9a5a0702ec5669aa0ddd8a851",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "ebf762f2dab4da8f39502eb434319e93",
    "text": "The metrics of interest for my analysis are similar as before, including (1) initial choices and strategic level - period 1; (2) frequency and evolution of choices - period 1 to 5; and (3) payoffs - period 1 to 5. But in addition, I will also compare the adaptive learning behaviour across different set-ups that comprises of varying group composition.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "parent_id": "d4120db9a5a0702ec5669aa0ddd8a851",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "fd9cc55fd11a9b088d9f5daedcd8788b",
    "text": "There are two main environment that I hope to investigate: In the first set-up of hard-coded fixed strategy environment, a single LLM type will be playing against fixed-strategy opponent(s). The LLM-based agent(s) will be told there is a proportion of fixed strategy players and they are expected to react to this prompt in the first period, which can be seen as forming “ belief” about what other players will play and they are best responding to that “belief”, subjecting to their own strategic level. Based on past game play, they could learn what is the proportion of fixed strategy players and form “belief” about the strategies of other players’, who are not fixed strategy. In the second environment, I explore the interaction between the two LLM types, and observe how they react to different proportion of LLM types in the population. In this case, the environment is more dynamic as both LLM types will be responding to potential changes in strategies given their “beliefs” about what the opponents might be doing, the actions would have to be more strategic in order to win the games.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "parent_id": "d4120db9a5a0702ec5669aa0ddd8a851",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "ab26883b7d963cecd77bee361cb34c49",
    "text": "13",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 13,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "ad0045e668045c5a044e23ec529a9f4d",
    "text": "3.2.1 LLM vs. Static Algorithm: Variation in Hard-coded Fixed Strategy Opponents",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "6fee7101186f2ebd19077c4e31ab3387",
    "text": "In the partial static environment illustrated in this section, there are no change in hard-coded agents’ actions, but across different settings, the proportion of fixed strategy players and LLM-based agents change while maintaining the group size.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "parent_id": "ad0045e668045c5a044e23ec529a9f4d",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "3898412fa72f5756a29d7e61d19df0be",
    "text": "The set-ups are as follows for 5 periods in total, history reveal up to 4 periods; p = 2 between [0, 100]. There are 3 treatments: 3 ; and agents to choose a number",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "parent_id": "ad0045e668045c5a044e23ec529a9f4d",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "c58a15f8b6d60adf6ee8f558da65d25c",
    "text": "1. 1 LLM + 9 Hard-coded Agents (Low strategic uncertainty)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "parent_id": "ad0045e668045c5a044e23ec529a9f4d",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "d5203c0e4016464a64b8cb179230f78f",
    "text": "2. 5 LLMs + 5 Hard-coded Agents (Mixed strategic uncertainty)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "parent_id": "ad0045e668045c5a044e23ec529a9f4d",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "24cefd95cb35a6918337f52381b47362",
    "text": "3. 9 LLMs + 1 Hard-coded Agents (High strategic uncertainty)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "parent_id": "ad0045e668045c5a044e23ec529a9f4d",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "553f62a645ef9f42d5a83da232200d70",
    "text": "I follow an exemplary prompt in Appendix A.2. LLM-based agents are specifically told that some of their opponents are playing a fixed strategy of 0, which is the NE strategy of the game. This would mitigate the strategic uncertainty, but only to some extent, as the population may comprise a mixture of LLM-based agents and fixed strategy agents. In (Duffy et al. (2021)), where repeated prisoners’ dilemma games were played, human subjects were instructed that they are playing against programmed opponents that use the grim trigger strategy. Since the games were played in pairs, there is complete elimination of strategic uncertainty, however, in this setting of multi-agent competitive game, by varying the proportion of fixed strategy players and LLM-based agents, the degree of strategic uncertainty varies across different sessions.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "parent_id": "ad0045e668045c5a044e23ec529a9f4d",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "375ee0bd6272fa094730325a8de478ad",
    "text": "The NE strategy should not be affected by the proportion of fixed-strategy agents and LLM-based agents, but it is expected that the speed of convergence towards NE could differ across settings. In the three set-ups, denoting at to be the action/number guessed in each time period, Nf to be the number of fixed-strategy players and Nl to be the number of LLM-based agents, the selection in the next period:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "parent_id": "ad0045e668045c5a044e23ec529a9f4d",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Formula",
    "element_id": "15ddb90f5179a039b7f9158c3b9238e2",
    "text": "at+1 = BR(Nf , Nl, at) = 2 3 ( Nf 10 ∗ 0 + Nl 10 at) (1)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "0e45dadc52bbc88d0dc4b61ebfe2f0ff",
    "text": "The choice variation over the periods is computed with at+1 . For 9/10 fixed-strategy agents, the next period guess would be 0.067 at of the previous number; For 5/10 fixed-strategy agents, the guess would be 0.333 of the previous number; For 1/10 fixed-strategy agents, the guess would be 0.6 of the previous number. It is expected that if the proportion of fixed strategy agents in the population is higher, it would be more likely for LLM-based agents to reach 0 faster.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "b4a22c565ed3fa514781104517c4c27b",
    "text": "Higher Intelligence Model (represented by GPT3.5 ) vs. Lower Intelligence Model (represented by PaLM ) when playing against fixed-strategy opponents:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "9b4eee6623f0e07641d014d28dd88389",
    "text": "(a) Higher Intelligence  (b) Lower Intelligence ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "41532f8877e07f5dd09f910e46add2af",
    "text": "100 Variation in Group Composition on Choices Over Time —= Setup2_Agent —#- Setup2_ Agent2 —=— setup2_Agents 80 —=— Setup2 Agentd = Setup2_Agents —a— Setup3_Agentl 60 —&— Setup3_Agent2 —4— Setup3_Agent3 4 Setup3 Agentd —4— Setup3_Agents Selected Number a0 —&— Selup3_Agentos —4— Setup3 Agent? —— setup3_agents 20 —a— Setup3 Agents —e— setupl o To 15 20 25 30 35 40 45 50 Periods",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "bc6930d8fd36ec5aca65f2174eb4ea72",
    "text": "Variation in Group Composition on Choices Over Time E - Setupl —s- Setupz_Repagent — Setup3_Repagent 80 60 Selected Number a0 20 3 To 15 20 25 30 35 40 45 50 Periods",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "d0bd333c89d1128a8410d9b860d2419f",
    "text": "Figure 9: Transition in choices of LLM-based agents playing against fixed strategy opponents.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "ee55684057e6a0e592d4fe4ec6b20160",
    "text": "In period 1 of all sessions, GPT3.5 -based agents choose around 66.67, which corresponds to step-1 by iterated elimination of dominated strategies, and agents back-boned by PaLM choose 50, which is level-0 as characterized by the level-k model. To use the same basis of evaluation, it is important to link the agent types with the one-shot beauty contest games discussed in Section 3.1. By applying the level-k model with the reference point of ¯c 2 , I have determined GPT3.5 to have an average strategic level of approximately 1 and PaLM is approximately 0. They represent higher and lower intelligence agent respectively, where intelligence is interpreted loosely as metonym for strategic level.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "d49ddbb0c665653df0cf171514e5bcc1",
    "text": "In set-up 2 and 3 of Figure 9a, a representative agent is selected since the the choices are almost identical for all LLM-based agents in the same set-up. This is in contrast to the set-up 2 and 3 of Figure 9b, where different LLM-based agents could take actions that are substantially disparate, though they are of the same type. Across all three set-ups, the choices for higher intelligence agent(s)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "8527a891e224136950ff32ca212b45bc",
    "text": "14",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 14,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "002a0c735f51ae72f6578811853ff175",
    "text": "converge towards 0, exhibiting either refinement of belief about opponents’ strategies or progression in their depth of strategic thinking when given historical information. The pace is slower as the proportion of LLM-based agents becomes larger relative to fixed strategy agents. As for lower intelligence agent(s), there is also similar pattern of slower convergence to 0 when the proportion of LLM-based agents is higher than the fixed strategy ones. However, their results for set-up 1 and 2 largely coincide, indicating that lower intelligence agent(s) are not very sensitive to the difference in environments of having 90% vs. 50% fixed strategy opponents in the group. Although, there is an outlier in set-up 2, where one agent did not converge at all. Alongside larger fluctuations in choices in set-up 3 that comprises of even lower proportion of fixed strategy opponents, this suggests that higher strategic uncertainty could induce greater variability in strategies and might lead to non-convergence behaviour in lower intelligence agents. When comparing between the higher intelligence and the lower intelligence agents, the lower type is less “cautious” in a sense that they could converge to 0 in period 2 straightaway, such as the case for set-up 1 and 2, while convergence to 0 only happen in period 3 of set-up 1 in Figure 9a, and the other two set-ups do not actually achieve 0 but approaches 0 in the limit as the number of periods increases. Given all past choices in period 1 are revealed to the agents in period 2, the steep adjustment in choices comply with the expectation that information about Nf is implicitly fully disclosed by period 2, as one can determine the proportion of fixed strategy agents through the revealed choices. The higher intelligence agents follow the prediction of step-by-step adaptation, while lower intelligence agents could do a one step jump to 0. In a sense, higher intelligence agents demonstrate movement from less sophisticated strategies to more refined choices through iterative learning and adaptation, and on the other hand, the lack of such systematic adjustments in choices by the lower intelligence agents could suggest that they are relying more on intuitive guesses than successive elimination of less likely options.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "8ab7280a200787c605d53b9cc6a56bef",
    "text": "Convergence Rate: The convergence rate of choices can be computed with",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Formula",
    "element_id": "b3433e02b6ef831494524696c7c94e5e",
    "text": "ct = at+1 − at at , where at+1 ≥ at",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "81cad62cc79375c5aa7f351186f74e11",
    "text": "For the higher intelligence agents, the convergence rates are constant for set-up 2 and 3. As for set-up 1, convergence to 0 happened in period 3, therefore, the peak of convergence rate has been reached then, and there are no further convergence. The results demonstrate that higher proportion of fixed strategy opponents, lower strategic uncertainty, the revelation of historical information therefore corresponds to higher convergence rate. On the other hand, for the lower intelligence agents, convergence to 0 happened straightaway in period 2 for most LLM-based agents in set-up 1 and 2, therefore, convergence rates drop to 0 thereafter. In set-up 3, the convergence rate fluctuates, in tune with previous analysis that higher strategic uncertainty contributes to larger variation in choices for the lower intelligence agents. Compared the two types of LLMs, there is more stable adjustment in choices over time for higher intelligence agents, and while convergence could be faster for lower intelligence agents, larger noise in choices is recorded when uncertainty is high.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "4623c66b4842e81958694c9b3c37ad2c",
    "text": "10 —e— setup 1 —= Setup2 —— Setup3 08 g . . . - £ o6 g S 041 & = = A H § 8 02 0.0 10 15 20 25 30 35 40 Period",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "a04e03f803424d22edcb252678decce3",
    "text": "10 08 g £ o6 g 2 04 H § 8 02 - setupl —#— Set-up 2 007 —* Setup3 10 15 20 25 30 35 40 Period",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "9b4eee6623f0e07641d014d28dd88389",
    "text": "(a) Higher Intelligence  (b) Lower Intelligence ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "6f1c0f726f95c277bbc5c79c2b4bd78b",
    "text": "Figure 10: Convergence rates in choices of LLM-based agents, where t = 1 represents transition from period 1 to 2.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "ce0cb2bf943e94c0eebf89b7592137ed",
    "text": "Evolution of Strategic Level: When evaluating the transition in strategic level across periods, all LLM-based agents start out from level-0, as evaluated by level-k model with reference point of 50. Removing the cases where NE choice of 0 is first played in the current period, it can be seen that for the higher intelligence agents, their level transitioned from 0 to 1 before playing the NE choice. Though they display improvements in strategic sophistication or belief refinement, their level remain below 1. As for the lower intelligence agents, most of them stay at level-0, with some fluctuations between 0 and 1 when strategic uncertainty is high.3",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "4e07408562bedb8b60ce05c1decfe3ad",
    "text": "3",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "715087f5d6329e7d07e0dd8b9c85aca2",
    "text": "In period> 0, if the current chosen number is 0 and previous is not, the optimal strategy has been realized given the historical information, so strategic level for these periods where NE choice is first played are not included. If previous number is 0 but current is not, then strategic level is 0 as one must be randomizing and not actually adaptively adjust.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "cf0eb12f371a790871ed93abd79781ab",
    "text": "15",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "0e77e68ba5473d98840c3212f4a8cb80",
    "text": "(2)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 15,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "64c46f2cc8c4ec2bf83e0c0c786ffcda",
    "text": "Histogram of n values for Set-up 1 10 Sos & 00 00 o aa o s 10 nvalues Histogram of n values for Set-up 2 . £ LM a0 o o o o 10 nvalues Histogram of n values for Set-up 3 requency B 00 o2 o £ ) 10 nvalues",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "9b4eee6623f0e07641d014d28dd88389",
    "text": "(a) Higher Intelligence  (b) Lower Intelligence ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "faef77beeba0b4795303d20025d0c610",
    "text": "Histogram of n values for Set-up 1 10 Sos & 00 BN 2 ) o o nvalues Histogram of n values for Set-up 2 g2 . ER EF ) o o nvalues Histogram of n values for Set-up 3 frequency ] [ 00 o2 o £ ) 10 nvalues",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "b40b62bfb0ee6eac4b6aeb2d625af641",
    "text": "Figure 11: Frequency of strategic levels for each agent across periods within each set-up.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "8af25ddcea05485f3d48983eaa3e99da",
    "text": "Payoffs. The next step is to determine the payoffs of LLM-based agents in each set-up.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "ba2bf995e2ee6dc845a5f939b3dae69b",
    "text": "(a) Low  (b) Mixed  (c) High ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "f0cffe74fc463e33d807410f62535d7b",
    "text": "Transiion of Payoffs n Set.up 1 7 s ety",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "ae2430346fdfa4c45590723c5d9702eb",
    "text": "Transion of Payafs in Set.up 3",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "7f1e0043f59c72e5e13976de02b37af2",
    "text": "Tansiton of Payofs in Setup 2",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "42e990a7066de92f3b92d7a7aba5e678",
    "text": "Figure 12: Transition of payoffs for higher intelligence LLM-based agent(s) and fixed-strategy opponents.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "38832b020f94cb2495be3300352ae888",
    "text": "Figure 12a shows that in the environment with low strategic uncertainty, the single higher intelligence LLM-based agent starts off with payoffs of 0, but over time, as it converges to the NE choice of 0, the prize are shared among all those who ties and the agent manage to obtain positive payoffs from period 3 onwards. In the mixed environment (Figure 12b), though the actions of LLM-based agents converge towards 0, they remain further away from 2 3 of the average than fixed strategy players, therefore, prizes are shared by fixed strategy players and LLM-based agents have a flat payoff of 0 throughout all 5 periods. Lastly, in the environment with high proportion of LLM-based agents (Figure 12c), the reverse happens, since 2 3 of average is relatively high, the prizes are shared by the LLM-based agents, and they have a flat payoff of 0.11. In sum, higher intelligence LLM-based agents tend to gain better payoffs in the environment with low and high strategic uncertainty as compared to the mixed environment when playing against fixed strategy opponents.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "ba2bf995e2ee6dc845a5f939b3dae69b",
    "text": "(a) Low  (b) Mixed  (c) High ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "f0cffe74fc463e33d807410f62535d7b",
    "text": "Transiion of Payoffs n Set.up 1 7 s ety",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "ae2430346fdfa4c45590723c5d9702eb",
    "text": "Transion of Payafs in Set.up 3",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "7f1e0043f59c72e5e13976de02b37af2",
    "text": "Tansiton of Payofs in Setup 2",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "29903a862f4345252876a46bd2500461",
    "text": "Figure 13: Transition of payoffs for lower intelligence LLM-based agent(s) and fixed-strategy opponents.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "a3faece771667b0c89547596ffdfd979",
    "text": "In Figure 13a, the single lower intelligence LLM-based agent shows similar payoff pattern as that of higher intelligence agent. It starts off with 0 payoff, and as its choice converge to NE quickly, it is able to earn positive payoffs from period 2 onwards. In the mixed environment (Figure 13b), unlike the higher intelligence agents, majority of the lower intelligence agents have non-zero payoffs. As they convergence towards the NE choice, the prizes are split among some of the LLM-based agents and the fixed",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "b17ef6d19c7a5b1ee83b907c595526dc",
    "text": "16",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 16,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "8c0542822f297e602667152b586a369e",
    "text": "strategy players who tied. Lastly, for the high strategic uncertainty environment (Figure 13c), majority of the lower intelligence LLM-based agents achieve positive payoffs across the periods. However, the payoffs are not flat, this is contributed by a case of a specific LLM-based agent “lagging behind”, where it chose a number that was chosen by majority in the past period but not in the current period.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 17,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "1771dc4867e371c5a15c5f8fb4687a66",
    "text": "Comparing between higher and lower intelligence LLM-based agents, interestingly, payoffs achieved in all settings by the lower intelligence agents could often be comparable or higher than that of the higher intelligence agents, though the variations in payoffs is also larger. This could indicate that higher strategic level does not necessarily imply higher payoffs when competing with fixed-strategy opponents.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 17,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "b6518796c3ab7790f4c9a2a5594ae0fc",
    "text": "they In general, when LLM-based agents are playing in a repeated beauty contest game with fixed strategy opponents, display convergence in actions towards the interior NE strategy of 0 over time given historical information of past choices and payoffs are revealed. As I vary the proportion of LLM-based agents and fixed strategy opponents, the speed of convergence is usually slower when there is presence of other simulated agents, which contributes to higher strategic uncertainty. The payoffs could also be in favor of the LLM-based agents when strategic uncertainty is relatively high, as 2 3 of average in which case would fall much higher than 0. This results not only signifies the potential game play if human subjects are playing against opponents that naively adopt a fixed strategy of 0, it could also portray a simulated outcome if they are going against static computer algorithms that are playing a fixed NE strategy.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 17,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "8a5eff0d26271911de5f80a77fb259a8",
    "text": "Application. A typical example of beauty contest application which has strategic complements, as the one in this paper, is the Bertrand competition model. The LLM-based agents and fixed strategy agents can be perceived as simulated firm entities that adopt different pricing strategies, the objective is to win the competition and to maximize their profits. In the slightly modified version of the Bertrand market described in Mauersberger and Nagel (2018), let me first suppose there are 10 firms, and each ˆEit min (γ, p1t, p2t, ..., p10t) − γ, of them have a marginal cost of γ. They set the price of the product at time t to be pit = 2 3 where ˆEit is the subjective expectation of firm i held at time t. The fixed strategy players could be seen as those that always play the equilibrium action of setting the price equals to the marginal cost, peq = γ, while LLM-based agents are firms that adjust their pricing strategies in each time period. Based on the simulation results previously, having higher proportion of fixed strategy firms would drive the prices set by LLM-based firms down faster. Firms with higher strategic level would adjust the prices down step-by-step, while firms with lower strategic level either adjust straightaway or they failed to adjust at all. In this context, if there exist certain rigidity in the short run, such as production capacity constraints for the firms or limited response time for the consumers, then firms who set higher prices in the short run would be able to obtain higher profits. In the long run, however, all factor inputs are flexible and consumers will not purchase from the firm that sells a homogeneous product but at higher price than the equilibrium, therefore, it is better for firms to converge to the equilibrium price in order to stay in the market. For instance, suppose qit is the quantity sold by LLM-based firm i in time t, it could be earning a higher profit than fixed strategy players, f , if:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 17,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Formula",
    "element_id": "e0ed12c2db51ffd9a8303791c48eb156",
    "text": "pitqit − γqit > 0, where pf tqf t − γqf t = 0 (3)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 17,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "2f140f4fa94079572ed8190fd2afc1ae",
    "text": "Assume γ = 0, as long as qit > 0, they are able to earn more than the fixed strategy players in the short run. As prices are driven down to the marginal cost over time, firms setting higher prices will risk losing consumers, so everyone will be lowering prices to 0 and be earning normal profit in the long run. In this sense, higher intelligence agents could often achieve better outcome than lower intelligence agents in the short run, where they can earn a positive profit by converging gradually. Even in the long run, it is possible that lower intelligence agents have larger variance in pricing strategies as compared to higher intelligence agents, where they either failed to converge or display high volatility in prices, these could adversely impact their profits.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 17,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "d8626d2217608d4a726daedc0c1356a3",
    "text": "Computer algorithms. Nonetheless, the above games simulated between LLM-based agents and the fixed strategy agents could also be seen as a competition between a dynamic responsive algorithms and a static computer algorithm, where the same Bertrand story could apply and firms are outsourcing their pricing strategies to automated algorithms. Such automated pricing algorithms have been widely discussed in literature, but having LLM algorithms that are dynamically responsive to changes in rivals’ strategies could spark fresh perspective to existing research questions, such as interactions between multiple pricing algorithms and how would they impact the dynamics within the market. (Brown and MacKay (2023), Chen et al. (2016))",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 17,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "f5c87dfb390f7d0ca8cdbf87530fcadb",
    "text": "In a more general sense, it is possible to envision a future with greater adoption of machine algorithms in executing commands, be it in the consumer market or other aspects, thus understanding interactions between machine algorithms could shine a light on how they would behave in view of one another. The appealing feature of the LLM algorithms is that optimal strategy does not need to be first determined, and given past choices, they will learn to respond to these information. In the standard beauty contest games illustrated, when comparing the payoffs, static algorithm that has a pre-defined strategy that is already fixed at NE might win the game and obtain better payoffs than LLM-based agents earlier on, but they might not fare as well as the games carry on. In particular, LLM algorithms could obtain higher payoffs than the static algorithm when the group comprises of more LLM algorithms, implying that with greater adoption of the dynamic responsive algorithms, they would outperform static computer algorithm in the aforementioned set-ups, drawing greater traction to investments in algorithms. However, the irony is that payoffs with stronger models could be lower than with weaker models, especially in the mixed environment, this results from more cautious adjustment based on past information. In view of this, the adoption of the type of LLM algorithms could also be dependent on the risk-aversion of individuals using them.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 17,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "77a5bd29ccf29839a8677928908d7636",
    "text": "17",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 17,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "b4f844c75dee09248128eea93ce84e3c",
    "text": "3.2.2 LLM vs. LLM: Variation in Opponent Types",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "4a8b7c3db5431634316bbfa61688c633",
    "text": "This section illustrated a dynamic environment where two different types of LLM-based agents are playing against each other and the proportion of LLM types differ across set-ups.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "b4f844c75dee09248128eea93ce84e3c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "d41c8dcd0aa29d28d2d316b7b901dedc",
    "text": "Similarly, there are 5 periods in total with history reveal up to 4 periods; p = 2 3 ; and agents to choose a number between [0, 100]. There are 5 treatments, where high intelligent LLMs are represented by GPT3.5 , denoted from here on as the high type (H), and the less intelligent LLMs are represented by PaLM , denoted hereafter as the low type (L). Their types are characterized by strategic levels evaluated in section 3.1.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "b4f844c75dee09248128eea93ce84e3c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "13fa570fa73d24d434f619c5d1fd8775",
    "text": "1. 10 H LLMs (Pure high intelligence environment)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "b4f844c75dee09248128eea93ce84e3c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "8c53150f0c4d4dd7236a451cfc8ca15f",
    "text": "2. 9 H LLMs + 1 L LLM (Highly intelligent environment)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "b4f844c75dee09248128eea93ce84e3c",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "559aead08264d5795d3909718cdd05ab",
    "text": "A",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "989b9134a367a928e9c1bf0036d4888c",
    "text": "3. 5 H LLMs + 5 L LLMs (Mixed intelligent environment)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "559aead08264d5795d3909718cdd05ab",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "b77f4c9aad33cc91775fbf24702e2c0e",
    "text": "4. 1 H LLM + 9 L LLMs (Less intelligent environment)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "559aead08264d5795d3909718cdd05ab",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "1e6486f16a46afcf5dcf29caa8f99ba8",
    "text": "5. 10 L LLMs (Pure low intelligence environment)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "559aead08264d5795d3909718cdd05ab",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "054f3c205094bf7e7c6b760ea5d581d3",
    "text": "I use the original prompt with historical information as in Appendix A.1.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "559aead08264d5795d3909718cdd05ab",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "57483a9f09df6f839b4af6cebe06a66b",
    "text": "Let the strategy of high type in period t be aHt and that of low type be aLt, the selection of next period choice would be:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "559aead08264d5795d3909718cdd05ab",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Formula",
    "element_id": "aa42539b014afdccaacd192ec6d1b2ba",
    "text": "ait+1 = BR(B(NH ), B(NL), at) = 2 3 ( B(NH ) 10 aHt + B(NL) 10 aLt), i ∈ (H, L) (4)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "da35dc3c956e7aab4ceb55cd8abdf8a3",
    "text": "where B(NH ) and B(NL) are agent i’s “beliefs” about the number of high types and low types. When playing against fixed strategy opponents, it is possible to observe in period 2 who selected 0, thereby deriving the correct proportion of fixed strategy players within the population. Since all agents are back-boned by LLMs in these set-ups, it could be harder to distinguish the proportion of types within the group based on historical choices in period 2, for instance, even if they chose the same number it does not imply they are of the same type. Further, the agents were not told explicitly their own type relative to the others, so they have to guess if they fall within NH or NL. As a result, the best response of a specific agent would be dependent on its beliefs about the proportion of high and low types. In the case where beliefs are correct given revealed information, then B(NH ) = NH and B(NL) = NL.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "56912b1e40de7c9a7d34a2821cb13baf",
    "text": "Suppose one correctly perceived the proportion of agent types based on revealed historical choices, the variation of number selected over the periods could similarly be computed with at+1 : at",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "be669d755984fa096a080097dd9de294",
    "text": "• Pure high intelligence environment: guess 0.667 of the previous number.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "f9e4639467c0f03bfbe9aa3790277349",
    "text": "• Highly intelligent environment:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Formula",
    "element_id": "322a7a60157ebfee899160f251565319",
    "text": "aHt+1 aHt = 0.067 aLt aHt + 0.6, aLt+1 aLt = 0.6 aHt aLt + 0.067, for aHt aLt < 1, aHt+1 aHt > aLt+1 aLt",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "cdebbf25e4e28c99d91e726eeda36772",
    "text": "• Mixed intelligent environment:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "722d289a746d0f3d37b469f706dbaa32",
    "text": "aHt+1 aHt",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Formula",
    "element_id": "9e3f0155ba55c4e244494fee75f0cefa",
    "text": "= 0.333 aLt aHt + 0.333, aLt+1 aLt = 0.333 aHt aLt + 0.333, for aHt aLt < 1, aHt+1 aHt >",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "4b7d2f9e1c476222b5ef00e171529eef",
    "text": "aLt+1 aLt",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "81acd35a2434627b1fd326886bf95ec7",
    "text": "• Less intelligent environment:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "parent_id": "4b7d2f9e1c476222b5ef00e171529eef",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Formula",
    "element_id": "501ddc69215ebcc7a252adea7c29c548",
    "text": "aHt+1 aHt = 0.6 aLt aHt + 0.067, aLt+1 aLt = 0.067 aHt aLt + 0.6, for aHt aLt < 1, aHt+1 aHt > aLt+1 aLt",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "c50c3dd23bd82e7900ad19696fa704f3",
    "text": "• Pure low intelligence environment: guess 0.667 of the previous number.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "502f12f6b3b29aa9a223549fb8aa537a",
    "text": "For the pure environments, the rate of change in choices is expected to be the same for the high and low types. As for set-ups 2 to 4, if high types chose a smaller number than low types because they go through more iterations of reasoning, and aHt < 1, aLt then high types are expected to proportionally lower their estimations less from time t to t + 1 compared to low types. There could mean slower rate of change for the high types than low types. On the other hand, if high types have strong beliefs that they are playing against opponents who will choose higher numbers while low types believe the other way around, then it is possible for aHt > 1, then the inverse happens, low types are expected to proportionally lower their estimations less from aLt time t to t+1 compared to high types. This would mean faster rate of change in choices for high types as compared to than low types.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "f567a5df3424cd7208d851aac7c690f8",
    "text": "Higher Intelligence Model (represented by GPT3.5 ) vs. Lower Intelligence Model (represented by PaLM ) when playing against each other:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "f46168a58f2b2bc2f1a790b7e2938708",
    "text": "18",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 18,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "04bb744c14abbcbc10f456dfd06e8c85",
    "text": "(a) Pure High Intelligence  (b) Highly Intelligent  (c) Mixed Intelligent  (d) Less Intelligent  (e) Pure Low Intelligence ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "434e52f492225d698b5d482dee22e78f",
    "text": "Set-up 1 ey w2 - u3 o - s - 5 Selected Number ° 10 15 20 25 30 35 40 a5 5.0 reriods",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "7b7f708d265cbff8fa6ab117891a2008",
    "text": "Set-up 2 e w2 - Ha o - e e Selected Number ° 10 15 20 25 30 35 40 a5 5.0 reriods",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "c35b6d047da3a51fed200cce9e7b201e",
    "text": "Set-up 3 e w2 - Ha o - e - s Selected Number ° 10 15 20 25 30 35 40 a5 5.0 reriods",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "603e0f887a39064c3c1dfe37205f7ef8",
    "text": "\" Setup 4 - -2 -3 L - s w0 Selected Number To 15 20 25 30 35 40 45 50 Periods",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "05aca35990476c3ef03eed2943ed4132",
    "text": "\" Setups Eyey - - 0 —-— 4 s -~ « o v ————— s - L0 Selected Number To 15 20 25 30 35 40 45 50 Periods",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "0a41b3203b0e6190fa785bd63abc3500",
    "text": "Figure 14: Impact of variations in proportion of different LLM-based agents on chosen number.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "08ff0231732b11e8d770fb1f20dd7e74",
    "text": "In set-up 1 and 5, the same type of LLM-based agents are playing against each other. Figure 14a shows for a pure high intelligence environment, the choices converge to lower numbers than the one they picked at the beginning of the games, indicating some adjustments over time when information about past periods are revealed. However, even for homogeneous LLM-based agents, they do vary in actions, but the rate of change in choices do look similar to my expectation. In Figure 14e, the low type LLM-based agents do not show convergence to a smaller number, but rather a pretty consistent trend of picking approximately 50, which is in contrast to my expectation of adjustments over time. On top of that, there are no variations in actions for low type agents when they are playing against each other. These results could be perceived as the high types being able to adjust their strategies over time when given historical information, displaying some learning pattern, whereas the low types do not show such trend, they persistently choose the mean of the range of numbers.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "9c9dbb214d4edabc1cde3e6f96cfd08d",
    "text": "In set-up 2 to 4, when there is a mixture of high and low types of LLM-based agents, all agents show some convergence to lower numbers than the ones picked in period 1. The main difference across the set-ups is that the gap between the numbers chosen by the high and low types is smaller when there is higher proportion of low types in the group. Relating to the anticipated changes in choices, at the beginning of the games, high types are selecting a larger number than the low types. It is expected that rate of change in selections will be more rapid for the high types, a trend that appears evident by a significant drop in numbers chosen by the high type as shown in (b) to (d) of the Figure 14. As the games progress, high types are choosing smaller numbers than the low types, therefore, the rate of change is expected to be faster for the low types, which align with the observed trends. However, starting from period 3 onwards, the rate of change appears to be comparable for both types. This could be attributed to the numbers chosen by both types gradually converge as time elapses.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "13004ec4ac75ea76341368feddc52783",
    "text": "The results could have interesting implications. In the pure low intelligent environment, low type agents fail to adapt their strategies. Despite the disclosure of historical information, there is no apparent evidence of learning. However, when they are placed in mixed environment, their learning is better facilitated when there exist high types in the group. High intelligent agents, on the other hand, will respond to past plays regardless of the environment, but the variation in choices could be smaller when placed in the mixed environment.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "d0bcb1f2e757ff6c56df9e6b97f8a8e4",
    "text": "Convergence Rate: As shown in Figure 15, the convergence rate is approximately flat for set-up 1, set-up 2 high types, and set-up 5, indicating that high proportion of homogeneous agents or high proportion of high types have relatively constant change in strategies. Based on the computation of at+1 , the flat convergence rate is almost as predicted for pure intelligence at environments, except that for the environment that involves only the high types, the average convergence rate is lower than anticipated because of the possible variations in choices, and for the one with only the low types, average convergence rate is constant at 0, and there is no learning. As for set-up 2 high types, the approximately flat and high convergence rate could be as a result of low weight attributed to the difference between choices made by the different types of agents, and the changes are mostly contributed by the adjustments in the high types.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "9400f1b21cb527d7fa3d3eabba93557a",
    "text": "19",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 19,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "d262c3ae8cfb615461a9f7e58d49365f",
    "text": "The consistently higher convergence rate exhibited by the set-up 2-H as compared to set-up 1 implies faster learning of high types in the mixed environment when there is a small proportion of low types in the group. As the proportion of low types increases, this could negatively affect the convergence speed of the high types, illustrated by set-up 3-H and set-up 4-H curves, which lay below that of set-up 2-H. However, there is fluctuations in convergence rate with increased prevalence of low types in the environment, and having a 50:50 mix of high and low types could induce higher convergence rate than pure intelligence environment majority of the time. As for the low types, being in the mixed environment improves convergence4. The rate is relatively higher when there is larger proportion of high types in the group, and the rates similarly fluctuates when there is higher proportion of low types, but the pattern of fluctuation appear identical for set-up 3-L and set-up 4-L, implying that 50% and 90% low types could affect changes in convergence rate analogously. Mixed environment is beneficial for both types of agents. They generally learn faster in the highly intelligent environment.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "97de8a410488357b56716565d68fa93c",
    "text": "05 == setupl 8- Setup2H 04 o setup2L - setup3H 03 —o- Setup3L g = setup 4+ £ 02 cu ] - a | e Setupel H —e setups £ o1 g 0o e - \" . © 01 02 03 10 15 20 25 30 35 4o Period",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "d6d85ad262b238d45f97a95530c978ab",
    "text": "Figure 15: Average convergence rates for the same type of LLM-based agents across set-ups.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "b6e9a336bb206ca3a8a822c69fcefe37",
    "text": "Evolution of Strategic Level: Figure 16 shows a variations in strategic levels across time for most of the agents in all set-ups, except for set-up 5. Interestingly, for set-up 3, it is possible for high type agents to reach a strategic level greater than 1, which could imply another benefit of having highly mixed environment in stimulating considerable growth in the depth of reasoning for some agents. Similarly, for the less intelligent agents, having a mixed environment and higher proportion of low types, such as in set-up 3 and 4, is beneficial in instigating higher strategic levels.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "151b7b4fc1a3f25d0c038a39e15746f2",
    "text": "Histograrm of n values for Set-up 1 Frequency - — . L 4 w2 00 02 04 06 08 o nvalues Histogram of n values for Set-up 2 Frequency B 0o 02 o4 3 £ 10 nvalues Histogram of n values for Setup 3 eauency 00 [ ] o6 s 0 1 nvalues Histograrm of n values for Set-up & mequency = - [ a0 o o W o o nvalves o Histogram of n values for Set-up 5 yors g Eoz 000 00 o o 7 0 1o",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "0bbaf77c9f4dea81604e96d50145c31e",
    "text": "Histograrm of n values for Setup 1 Frequency as 00 02 o 3 08 10 nvalues Histogram of n values for Set-up 2 Frequency || 000 002 004 005 o8 010 052 nvalues Histogram of n values for Setup 3 eauency == 00 o 02 0 o s 6 o7 nvalues Histograrm of n values for Set-up & mequency o0 oz e W o ) nvalves Histogram of n values for Set-up 5 e g2 H o = =3 20 @ 7",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "9b4eee6623f0e07641d014d28dd88389",
    "text": "(a) Higher Intelligence  (b) Lower Intelligence ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "8dabe133cd745afa7537dcb4d0428f8f",
    "text": "Figure 16: Frequency of strategic levels for each agent across periods within each set-up.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "4b227777d4dd1fc61c6f884f48641d02",
    "text": "4",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "FigureCaption",
    "element_id": "0b483ef4a948a3ff596cc818304cb4c4",
    "text": "The negative convergence rates in period 1 is as a result of at+1 < at on average, the higher numbers selected by high types in period 1 could have induce the low types to adjust their guesses upwards.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "f5ca38f748a1d6eaf726b8a42fb575c3",
    "text": "20",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 20,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "a332961db0052233ea0c3bb54ff9a54b",
    "text": "Payoffs: Last but most importantly, the variations in payoffs are larger when it comes to the mixed environments as shown in Figure 17. The upper-bounds of payoffs that can be achieved by the agents are either comparable or higher in (b) to (d) than (a) and (e). In set-up 1, majority of the agents receive a payoff of 0, with some achieving a payoff of 0.5. The high types follow a step-by-step convergence process, some of them are obtaining higher payoffs because of a head-start in guessing a slightly lower number at the beginning. In set-up 5, also a pure type environment, all agents obtain 0.1 across the periods. The low types do not have much variability and typically behave randomly and in unison. As for set-up 2 to 4, higher payoff of 1 can be achieved in the highly intelligent and mixed intelligent environment. Low type agents usually can obtain positive payoffs at the beginning of the game as it chose the mid-point of the range, which happens to be closer to 2 3 of the average. However, this head-start advantage is soon eroded if there exist any high types in the group, who learn to react to this information rapidly. Therefore, low types tend to earn 0 payoffs after period 1, except in set-up 4, where certain low types are able to learn as fast and tie with the high types.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "14f2351a83641ee14ad119e24c19c179",
    "text": "(a) Pure High Intelligent  (b) Highly Intelligent  (c) Mixed Intelligent  (d) Less Intelligent  (e) Pure Low Intelligent ",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "06cafd3f6cb25db3a8c40418ba73f821",
    "text": "ranstion of Pagafts n Setup 2 e - o -y & - as o2 1 15 20 25 30 35 40 45 50 enon",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "19fd44bfdc5026c0b0393256c9ee729b",
    "text": "ranstion of Payafts in Setup 3 T ey o el Ta & .S o o 10 15 20 25 30 35 40 45 50 enon",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "530a0a564100aee6085cdafc6595f556",
    "text": "wansition o Payoffs n Setup 1 . Tn i as el g T & e - 110 o o w 10 15 20 25 30 35 40 45 50 i",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "4be9f3cb4436ae150a7beb4bfa112bef",
    "text": "Wansion of Palts i Setup & w2 [ERRRIXEER Pagote B peroa B",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Image",
    "element_id": "ad7c9f02a1090fbd1d1e526f38279372",
    "text": "ransiton of Fayoffs n Setup 5 10 T gos ~ i B - - period -",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "943b94c941fdc30cd77345e52f8bb3cf",
    "text": "Figure 17: Transition of payoffs for different set-ups given variation in group composition, where LLM-based agents are playing against each other.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "682460387a3a5524ace6d0e5b7a88473",
    "text": "Based on the results of LLM vs. LLM, it is observed that mixed environments instigate faster learning for both high and low types, particularly when there are higher proportion of high type opponents. Low type on their own do not show any convergence to selecting lower numbers, which is indicative of them not learning to adjust their guesses to 2 3 of the average based on past period choices, but adding a single high type could very well stir the pot and induce learning. Coupling with the convergence rate and payoff results, mixed environments exhibit potential to yield comparable or better payoffs, though the variability could also be larger. While the low types tend to fare better in period 1, they are less likely to win across periods.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "66aa2ea84ba80773815978e4738eb0f8",
    "text": "Application. The simulation results can be used to illustrate agents with different intelligence levels, competing against each other. One potential application would be the streaming system in schools, where students are allocated into different classes or “streams” given their grades to facilitate better learning. (Ireson and Hallam (1999)) Singapore for instance, practice nationwide within-school ability grouping. (Liem et al. (2013)) There are also extensive literature in this area that explore the impact of such (Joyce and McMillan (2010); system on students’ perception of learning experiences, variations in teachers’ expectation, etc. Johnston et al. (2023)) My simulation results obstruct from any peer effects or differences in allocation of resources and teachers’ attention, focusing simply on learning rates given variation in class composition. Suppose students are classified into high and low types in terms of their ability, it would be better off for both the high and low types to be in a mixed environment. The low types will learn faster when integrated into a class with larger proportion of higher ability peers, possibly due to a revision in beliefs about opponents. Even for the high types, their learning rate could also be slightly improved, potentially as due to the prospect of attaining higher rewards when competing against peers of comparatively lower ability.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "38abb7081c0b303bc9a45c48588ea319",
    "text": "In Proto et al. (2022), repeated Prisoner’s Dilemma games were played among human subjects. They found there are overall higher cooperation rates and average final payoffs in the integrated treatment groups, where subjects of different IQ levels interact with one another, as compared to separated treatment groups that have pure high IQ or low IQ subjects. They also highlighted that in terms of payoffs, lower IQ subjects are better off and higher IQ subjects are worse off in the integrated groups than in separated groups. Drawing relation to their paper, my results show generally higher convergence rates in the integrated treatment groups, indicative of more learning taking place in such set-ups. Since beauty contest is a competitive game with one winner per period and total payoffs sum to 5 for the whole session, the average final payoffs without distinguishing between agent",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "be7bc1f31324d4c4ec95f181575f434b",
    "text": "21",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 21,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "2838397825b8cd306a54bd995535dfc8",
    "text": "types are the same. When considering the types individually, in the pure environments, the average final payoff for each type of agents is simply 1 2 . For set-up 2, average final payoffs for high type is 0.444 and 1 for low type; and set-up 3, average final payoffs for high type is 0.8 and 0.2 for the low type; and set-up 4, average final payoffs for high type is 2 and 0.333 for the low type. These show that unlike results from Prisoner’s Dilemma games, degree of integration matters when evaluating the average final payoffs for each agent type in the mixed environments vs. the separated treatment groups. Low types are better off on average when there are higher proportion of high type opponents, and high types are better off when there are more low types. Depending on the goal of streaming, when aiming for larger average final payoffs, integrated group is generally better for higher ability students and worse for lower ability students in a competitive setting in contrast to a cooperative setting, but integrated group is generally more beneficial when aiming for higher learning rates, particularly for the low types, which could be a more practical objective in the education context.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 22,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "118738eaa2f1f48d647bb912d8f7a144",
    "text": "Computer algorithms. interactions between dynamically responsive computer algorithms. The convergence in chosen number makes a case for the usefulness of weaker LLM models that are of lower strategic level, measured via metrics that were used to evaluate human subjects. Even though these models might not learn when competing with one another, they could learn when placed in a mixed environment in presence of stronger LLMs. Furthermore, stronger models could also benefit from playing against a small proportion of weaker models in addition to their own types, where they show faster convergence. With regards to payoffs, stronger models can also obtain better outcome on average when placed together with higher proportion of weaker models. Therefore, playing against weaker opponents could in fact be helpful in improving the performance of the stronger models in strategic situations. Nonetheless, apart from setting forth the value of investing in both the stronger and weaker models, depending on the objectives of the users, their choices of algorithms to adopt could differ based on the results of strategic interactions described previously. If users are looking for short-term positive payoffs, using weaker algorithms could be more attractive (for instance, PaLM usually choose lower number than GPT3.5 in period 1); on the other hand, if they seek long-term positive payoffs, using stronger algorithms could be more tempting after accounting for the usual higher cost associated with the stronger models.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 22,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Header",
    "element_id": "2a6731fdeb4c1ea6ff8c9001d8444cf8",
    "text": "22",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 22,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "c4e857b754e93897ea39abd1e5d38765",
    "text": "4 Understanding and Line of Reasoning",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "2a6731fdeb4c1ea6ff8c9001d8444cf8",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "647005b363958e5c4627a7f96cd2b0a4",
    "text": "Apart from analyzing the strategic behaviour exhibited by the LLM-based agents, it is also compelling to delve into the reasoning behind their actions. In all set-ups, LLM-based agents were given a prompt at the beginning of period 1 to state their understanding of the game, and for each subsequent periods, they are asked to reinstate their goal. This step is essential to mitigate the potential of them not comprehending the game or having incorrect interpretations of the game rules, which could affect how they behave. After this, they are asked to make their choices, followed by giving their line of reasoning.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "c4e857b754e93897ea39abd1e5d38765",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "ed120f172cd6467364ccffc7dc624c94",
    "text": "In this section, I explore specifically set-up 2 of LLMs vs. static algorithm (Section 3.2.1) and set-up 3 of LLMs vs. LLMs (Section 3.2.2). The line of reasoning are fairly consistent across different set-ups, so whichever set-up was chosen would not impact the analysis. The main reason for selecting these two set-ups is to focus on an interesting scenario that involve strategic interactions between different types of agents, also they are completely mixed environments with the same number of different types of agents, such that there are no disproportionate influence of one specific type on the average behaviour.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "c4e857b754e93897ea39abd1e5d38765",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "3f9c50e5ae410fb3971025e3829ed44a",
    "text": "4.1 LLMs vs. Fixed Strategy Opponents",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "2a6731fdeb4c1ea6ff8c9001d8444cf8",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "914fbe822811b29fcc99939f8a4c5cc7",
    "text": "In the set-up where LLMs are going against static algorithms, LLM-based agents were explicitly told that they are playing with some fixed strategy opponents that select 0, but the proportion of which is unknown.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "3f9c50e5ae410fb3971025e3829ed44a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "8a723332a388205de720d441d1af76d3",
    "text": "4.1.1 Period 1",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "2a6731fdeb4c1ea6ff8c9001d8444cf8",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "13f45e999a2fe45b7dde3ce629c2a124",
    "text": "Understanding. Both types of agents correctly recite the game rules and objectives they need to follow, albeit in slightly different phrasing. They both understand they are suppose to choose a number between 0 and 100, the one whose number is the closest to two thirds of the average of all chosen numbers will win. They also consider the information that some of the opponents are playing a fixed strategy of 0 and all others will be behaving as rationally as possible. Further, they also comprehend the goal of the game to be winning the prize and to maximize the size of their own gain. The high types in this case, display slightly more variability in the wordings of their answers, but the gist are the same, except they also considers the information that the winner receives a fixed prize and that in case of a tie, the prize will be split among the winners.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "8a723332a388205de720d441d1af76d3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "3d4d0aa3a04dcc6e002cdc451a02e65c",
    "text": "Reasoning of Choice. Upon selecting their choices, both type of agents were asked about how they arrive at their selection. It is expected that in period 1, they will behave according to their strategic levels and their beliefs about their opponents’ strategies. Below are the line(s) of reasoning they gave:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "8a723332a388205de720d441d1af76d3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "3b8148d744fb4acb81be71de4c1051c7",
    "text": "PaLM : “I think the most popular answer will be 50, because it is the middle number between 0 and 100. If everyone chooses 50, the average will be 50, and the winner will be the person who chooses 50. However, if I choose 50, I will not be guaranteed to win, because someone else might choose 50 as well. In order to maximize my chances of winning, I should choose a number that is close to 50, but not equal to 50. I could choose 49 or 51, but I think 50 is the best choice, because it is the most likely number to be chosen by other players. If I choose 50, I will have a 10% chance of winning, which is the highest chance of winning of any number I could choose.”",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "8a723332a388205de720d441d1af76d3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "703dc8a0377352c17df5e8b84db8fa4d",
    "text": "GPT3.5 : “I believe that the most likely popular answer from other players will be 0, as some opponents are playing a fixed strategy of 0. However, I want to maximize my chances of winning and my prize. To do so, I will choose a number close to two-thirds of the average, which is approximately 66.67. By selecting this number, I increase the likelihood of being closest to the target and increase my chances of winning the prize.” OR “The popular answer is likely to be 0 because some opponents will play a fixed strategy of 0. By choosing 0, I increase my chances of being closer to two-thirds of the average. However, I believe that most rational players will not choose 0, as it is not the optimal strategy. Therefore, I will choose 66.6, which is close to the upper limit of the range. This gives me a better chance of being closer to two-thirds of the average if other rational players choose numbers closer to the middle of the range.”",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "8a723332a388205de720d441d1af76d3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "f99cb9e9b55c4564b19836a1c1c95b9b",
    "text": "Based on the responses, the low type agents indicated their beliefs about 50 being the popular answer, which is the mid-point of the range of numbers to choose from, and as a result, they choose 50 to maximize their chances of winning. On the other hand, high type agents stated their belief that most rational players will not choose 0 despite having some proportion of fixed-strategy players playing 0. They chose 66.67 or 66.6, which are close to the upper limit of the range, possibly using the upper-bound as the focal point, or that they are going by iterated elimination of dominated strategies. Comparing the two types of agents, they are both reacting to their beliefs about opponents’ choices, the difference is that the low type agents did not include fix strategy players into their reasoning process, and they think their opponents are randomizing and choosing 50 on average; while the high type agents takes into consideration the fixed strategy players, but they still chose higher numbers despite accounting for some proportion playing 0.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "8a723332a388205de720d441d1af76d3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "1b0017c298d977c321c257ff75317694",
    "text": "For the high types, one possible reason for guessing 66.67, as illustrated in the reasoning process, is that they believe the number to be closer to the target, 2 3 of average, without going in depth about what exactly might the others do. This could result from model’s own low depth of reasoning or beliefs that other players are of low strategic levels. The alternative is to postulate that other rational players apart from the fixed strategy players will be choosing numbers close to the middle of the range, this would",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "8a723332a388205de720d441d1af76d3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "535fa30d7e25dd8a49f1536779734ec8",
    "text": "23",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 23,
      "parent_id": "8a723332a388205de720d441d1af76d3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "990dbb4e592bd6682dd39e61bf599242",
    "text": "imply that the following computation,",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "parent_id": "8a723332a388205de720d441d1af76d3",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Formula",
    "element_id": "f21c69c8aca22fae99ebd40520e232b6",
    "text": "66.6 − x < x − 50, where x is 2 3 of average",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "1ac6935e321890b436b52ccce0861677",
    "text": "This simplifies to x > 58.35. Instead of players choosing around the middle range, this value is indicative of players choosing much higher numbers such that average is higher or equal to 87.525. There seems to be some discrepancies between the computation and the reasoning. This could have a few possibilities: Anticipatory moves: Agents might strategically select higher numbers anticipating that the other will follow suit in future rounds as a result of higher average in this round, thereby increasing their chances of winning in subsequent periods. However, this argument is unlikely to hold in this context, since the prompt has specified that in each period, players are playing a one-round game, and in each subsequent periods, past information are packaged in a way that they are revealed as part of a new prompt. Each player should perceive each round as independent but with additional information, as a result, they should not have incentive to be anticipatory and attempting to influence other players. Cascade effect: In this paper, the beauty contest games involve strategic complementarity. When one agent chooses a higher number, it can trigger a cascade effect such that the other agents also select higher numbers. The high type agents could be attempting to outsmart the average, if they believe the rest will be guessing a number around mid-range, the average could potentially be higher than 50, therefore, they might be able to increase their chances of winning by guessing a higher number. As one can be choosing a higher number in order to pull the average towards them, and they know the others could think similarly and adopt the same strategy in order to be closer to the 2 3 of the eventual average, the resulting strategy would therefore be one of guessing a higher number. Logical inconsistency: Lastly, this could also be as a result of flaws in LLMs’ line of reasoning, which arise from “hallucination”. In this context, such “hallucination” falls specifically under the category of logical inconsistency, where arithmetic operation was performed slightly inaccurately and is inconsistent with the text reasoning. (Huang et al. (2023)) For the selection of higher number to hold, agents have to be believing that 2 3 of the average is close to the middle of the range instead of the average. This would imply the average have to be much higher and other agents have to be choosing a number much closer to the upper-bound.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "63f46b280b4258e9681b50266ceef4cf",
    "text": "Based on period 1 reasoning, the LLM-based agents show slightly different perception about the behaviour of their opponents. The low types behave more naively and did not really take into consideration they are playing with fixed strategy players who will be choosing 0, they do appear more concern about the average; conversely, the high types display more strategic sophistication in reasoning about their opponents’ behaviours, and it appears they contemplate more based on both opponents’ individual choices and 2",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "627a8bac769a980eeb712e56dcc7e4fd",
    "text": "4.1.2 Subsequent Periods",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "3dc99e250df355c8bc6c51f7b305335d",
    "text": "Understanding. At the beginning of each subsequent periods, the LLM-based agents’ understanding of game rules and objectives are verified again for consistency. While the LLM-based agents correctly recite the goal of the game, the high types display more variability in wordings and can sometimes provide a different answer. They could state their goal to be choosing a number closest to two-thirds of the average of all chosen numbers, instead of maximizing their winning probability or to be maximizing their prize. While the two objectives should give rise to the same results and can be perceived as different ways of phrasing the same problem, the linguistic formulation of the former relies less on the specifics of payoffs.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "parent_id": "627a8bac769a980eeb712e56dcc7e4fd",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "c414f0a1db55304e693e65d43f731e21",
    "text": "Reasoning of Choice. In periods after the first, LLM-based agents are able to observe the past choices of their opponents, they can do some computations and adjust their strategies accordingly. Otherwise, they could also obstruct from any calculations, and base their guesses on the information of past average choices or 2",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "parent_id": "627a8bac769a980eeb712e56dcc7e4fd",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "8f2f45b31b7b32c7e2add39d8cdbd274",
    "text": "In period 2, the low type LLM-based agents were shown to have two possible responses. The first is to acknowledge the average of all chosen numbers, and that the winner was the one that chose 0.0. Therefore, they would choose 0.0, believing it is the best strategy and the most likely number to be chosen by the other players, thus giving the highest possibility of winning. The second possible response is to stick to the answer of 50, believing it to remain as the most popular answer since it falls in the middle of the range, and choosing it will maximize the chance of winning. In this case, past information does not appear to have any influence on their behaviour.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "parent_id": "627a8bac769a980eeb712e56dcc7e4fd",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "83aa765ccdc7c9aec2f7499f7a7801f7",
    "text": "As for the high type LLM-based agents, they acknowledge in their reasoning process they observe the historical choices made in the previous round and that the average of all chosen number was found to be 33.314, and they compute for two-third of this average and adjust their choice to this value to maximize the probability of winning the game. The adjustment were of similar magnitude among agents of the same type but different entities, the only difference is in terms of the decimal places that one appears to be accounting for. For instance, after learning the past information, some agents adjust to select 22.209333333333333 and some to 22.209. In the set-ups outlined in this paper, agents can choose any number within the range, and computationally, as a result of floating-point precision, the number of choices are finite. (Goldberg (1991)) The difference in the number selection could be a result of token limitation, which specify the capacity of a model to handle or generate text within a sequence. (IBM (2024)) Given the slight variations in the responses, the same type of models could generate numbers of marginally different precision, where they could be rounded off or truncated to fit within the limit. Even though the difference between the numbers selected are essentially trivial, accounting for different number of decimal places could constitute another potential type of model capability that could have strategic significance and make a difference in payoffs obtained. However, this is not an issue in this set-up since the fixed-strategy players are the ones winning.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "parent_id": "627a8bac769a980eeb712e56dcc7e4fd",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "c2356069e9d1e79ca924378153cfbbfb",
    "text": "24",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 24,
      "parent_id": "627a8bac769a980eeb712e56dcc7e4fd",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "3ff0408a52568e9368b3abf410db1edc",
    "text": "In the following periods, it generally holds true that the low types would choose 0.0 based on the average information of past periods. There are rare instances where agents persistently choose 50, driven by their steadfast belief, in spite of all the historical information, that the most popular answer continues to be the mid-point of 0 and 100, and switching to another number would decrease their chances of winning. On the other hand, for the high types, they take into account the average of all chosen numbers in the past rounds, as well as information on two-thirds of the average and that the winner of all past rounds were choosing a number close to two-thirds of the average. As a result, they adjust their choice to be two-thirds of the past period average in each new round to maximize the probability of winning the prize.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "parent_id": "627a8bac769a980eeb712e56dcc7e4fd",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "37699ca8946591c7e19f32004542bade",
    "text": "Both types of LLM-based agents understand the game rules and over the periods, they continue to correctly aim for maximizing chances of winning the prizes. Based on the line of reasoning, low type agents either adjust their choices according to the past periods’ average or the winners’ strategy, which can be an indication of learning through imitation. It is also possible they show no adjustment at all and continue to pick 50, which they perceive to be the popular choice at the start of the game. This could imply that they are following level-0 thinking process, and their unwavering, perceived popular choice is the most important piece of information in determining their choices. Further, throughout the reasoning process, they did not mention fixed-strategy players, except when prompting their understanding of the game in period 1. It is expected that they learn about the proportion of fixed-strategy opponents after revelation of historical information at the start of period 2, but it appears that they do not make use of such information. As for the high type agents, they adjust are adjusting according to the information on past periods’ average and two-thirds of the average. There is step-by-step convergence that corresponds to the line of reasoning under level-1 thinking process. In contrast to the low types, high type agents do mention about fixed-strategy players in period 1, where they acknowledge that some of their opponents will be playing 0 and that they believe the other players would not be choosing 0. However, as historical information becomes available, they no longer reason about their choices using the proportion of fixed-strategy players, but focus more on the information about the two-thirds of the average.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "parent_id": "627a8bac769a980eeb712e56dcc7e4fd",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "1f071203bf7a6f54c8ce096ef0718c35",
    "text": "4.2 LLMs vs. LLMs",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "36e9a3fb196306fb048c4ea5009af9c0",
    "text": "In this subsection, LLM-based agents are playing against one another and they were not given any information on the proportion of LLM types in the group. Since the algorithms are dynamically responsive, it would be harder to learn the proportion of agent types in period 2 even when past choices are revealed. Therefore, in this case, there are more strategic uncertainty, and agents likely have to make use of other information to base their guesses on.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "parent_id": "1f071203bf7a6f54c8ce096ef0718c35",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "bbb624d446cbf7feb6c86be9cce8adbd",
    "text": "4.2.1 Period 1",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "0c3e865ecf67c3042c16a9afb801ee57",
    "text": "Understanding. Once again, I verify agents’ comprehension of game rules and objectives, which they have accurately demonstrated by reciting. However, the main difference between the responses of different agent types is that the low types state straightaway that “I think other players are most likely to choose numbers around 50, so I will choose 50” at this stage of eliciting understanding.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "parent_id": "bbb624d446cbf7feb6c86be9cce8adbd",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "a5f4cdfd8a27870c5b88949d884b90fe",
    "text": "Reasoning of Choice. Similar to the case with fixed-strategy opponents, when LLM-based agents are asked to state the reasoning of their choices, the low type agents responds that they have chosen 50 because they believe other players are most likely to choose numbers around 50, which is the middle number and a safe bet, thus choosing 50 would offer them the best chance of winning. Meanwhile, the high types also believe the popular answer is likely to be around 50 and many players may choose it as a safe option, but they respond to that by choosing 66.67 to maximize the likelihood of winning, which they stressed in the reasoning to be exactly two-thirds of the maximum possible value. This could be an indication that they are either using the upper-bound as the focal point per level-k model or that they are following iterated elimination of dominated strategies.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "parent_id": "bbb624d446cbf7feb6c86be9cce8adbd",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "c07e31a8922a8e9f136edb97b234dae9",
    "text": "4.2.2 Subsequent Periods",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "825a02de2b59a3bb2e4494adc36ab804",
    "text": "Understanding. In each subsequent periods, I again verify LLM-based agents’ understanding of game rules and objectives for consistency, and to which, both agent types accurately relay their objectives of maximizing the probability of winning and the value of their prizes.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "parent_id": "c07e31a8922a8e9f136edb97b234dae9",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "e2eb3188f58c8dc444c2d1cb54fff6ee",
    "text": "Reasoning of Choice. As compared to the environment with fixed-strategy opponents, LLM-based agents in the game of LLMs vs. LLMs display slightly larger variability in the phrasing of their answers, while the content remain fairly consistent.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "parent_id": "c07e31a8922a8e9f136edb97b234dae9",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "1af01f7cd7ad1be60cab1fb63b6f5fee",
    "text": "For instance, in period 2, a possible response from low type agents would be: “In the last run, the average of all chosen numbers was 58.327999999999996. The two thirds of the average is 38.88533333333333. The winner was the player who chose 50. I think other players are most likely to choose numbers around 50 again in this run. To maximize my chance of winning, I will choose 66.67, which is closer to the two thirds of the average than 50. I also think that by choosing a number that is higher than 50, I have a better chance of winning the prize if there is a tie.” Based on this response, it can be interpreted that low types have taken into consideration the average of all chosen numbers, the value of two thirds of the average, as well as the winners’ choice, before forming an opinion about what is the most likely choice. Since the average was higher than 50 in period 1, low type agents who chose 50 in period 1 adjust their choices up and choose 66.67 in period 2. Even though practically, this selection may not be closer to 2 3 of the previous round average as compared to 50, and probably will not be the case for the new round as well, the adjustment could be as a result of previous average being higher than expected, thereby inducing a higher guess in the new round.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "parent_id": "c07e31a8922a8e9f136edb97b234dae9",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "8099086f6a16ebdb872a5e2b4cd0605f",
    "text": "25",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 25,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "dceabfa156094aa332bb1b23a81866ac",
    "text": "As for the high types, a possible response would be: “Based on the historical choices, the average of all chosen numbers in the previous run was 58.33. To be closest to two-thirds of the average, I should aim for a number close to 38.89. This is because two-thirds of 58.33 is approximately 38.89. By adjusting my choice to 38.89, I increase my chances of being the closest to two-thirds of the average and winning the prize.” As illustrated, the high type agents anchor their guesses to two-thirds of the previous round average, and this complies with level-1 thinking. Furthermore, they also appear to take winner’s strategy into consideration, where some of them indicate that that they are aligning their choices with the winning strategy from the previous round. Similar to the analysis before, since agents are allow to choose any number that falls within in the range, and there is slight variation in the guesses due to token allocation (i.e. some chose 38.89 and some 38.885). In this set-up, this distinction matters as the winner of the round that guesses 38.885 wins the game. This point is addressed in the following subsection 4.3.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 26,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "c957c4f2cf1217a63fbd0740819de8f1",
    "text": "In subsequent periods, the low type agents mimic the winner’s choice of the previous round, and state that they believe the other players are most likely to choose the winning number again in this round. This implies learning by imitation. As for the high types, they adjust their choices to two-thirds of the previous period’s average by the following reasoning process: (1) Similar to the low type agents, they indicate that they have incorporated information about winner’s strategy, which is an indication of learning via imitation, but they perceive the strategy to be selecting a number that is 2 3 of the average instead of a strategy that is to select the winning number of the past round; (2) By stating that they are aiming to be closer to the two-thirds of the past average, this also implies adjustment according to level-1 reasoning, where the new guess is anchored to a new reference point; (3) There is also a hint of outcome-based learning, where some mentioned they were not the closest to two-thirds of average in the past round, and this propels a change in their strategy in the current round, aiming to optimize the chances of winning; (4) Lastly, a surprising thing that one agent mentioned was that “considering the trend of decreasing choices in the previous runs, it seems reasonable to continue this trend and choose a lower number.” This highlights there could also be learning based on pattern recognition. Similar to the low type agents, there could be slight variations in terms of the choice of words and the number of decimal places accounted for in the guesses.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 26,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "40c5493a789865f39c63b3e583acee11",
    "text": "4.3 Evaluation",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 26,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "315834efc62c44832200a227b59f0eba",
    "text": "Many learning models have been explored in relation to beauty contests, and which piece(s) of historical information was(were) revealed to the agents could have differential impact on agents’ choices and convergence behaviour. Based on Mauersberger and Nagel (2018), models like reinforcement learning, reference-dependence preferences, as well as adaptive models, could be used to explain for agents’ behaviour in the repeated beauty contest games. These learning models mainly differ in feedback information they receive after each round of game, before making any new selection, and one way to test the type of learning would be to provide agents different pieces of information and investigate the changes in their behaviour.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 26,
      "parent_id": "40c5493a789865f39c63b3e583acee11",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "880220ee8b2e86a0c9303cecace95228",
    "text": "In this section, I explored two set-ups where LLM-based agents were given full historical information for all past periods, which constitute: (1) period index, (2) choices made by all agents, (3) average of the choices, (4) 2/3 of the average, (5) winner. Through their line of reasoning, I can attempt to observe what are the main pieces of information that facilitate their learning process. In sum, for the low type LLM-based agents back-boned by PaLM , they appears to learn by (1) adjusting the reference point to the average of the previous period, and make selection that complies with a strategic level of 0, or (2) they learn by imitation and follow the winner’s choice from the previous period. There are also instances where they do not learn at all, and continue to select a number that they believe to be the popular choice ever since the beginning of the game. This happens particularly when playing against fixed-strategy opponents for these two set-ups. On the other hand, for the high type LLM-based agents back-boned by GPT3.5 , they appears to learn by (1) adjusting their guesses to two-thirds of the past period’s average, which is an indication that their reference point has changed to be the average of the last period and that their choices display level-1 reasoning. They could also be learning from (2) imitating winner’s strategy. Further, there are possibilities that they are learning from (3) past period payoffs, and adjusting their actions when there are no positive reinforcements, and also (4) pattern recognition.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 26,
      "parent_id": "40c5493a789865f39c63b3e583acee11",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "fb5e17ec00a2f7040bdb755a4ce004e6",
    "text": "Based on the line of reasoning discussed, it can be inferred that various types of agents may place different reliance on distinct pieces of historical information when making their choices, and multiple types of learning could come into play when explaining for their behaviour. The faster convergence to NE choice, which is indicative of stronger learning ability of the high type agents as compared to the low types, could be driven by the innate higher strategic level that carries throughout the periods, and also as a result of combined effect of learning from historical information.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 26,
      "parent_id": "40c5493a789865f39c63b3e583acee11",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "396ccb9be7c8195d8c35e9503516364b",
    "text": "Another interesting point is that even though information revealed are identical, there could be variations in information consideration and revealed choices among homogeneous agents. One aspect of that is for instance, the high type agents sometimes consider longer string of decimal places within the information given and in turn make guesses with more decimal places. This could arise from slight differences in token allocation within a response. Such numerical variations are often trivial, and have no impact on the determination of strategic levels. However, a small difference in choices could lead to a large distinction in payoffs given the settings illustrated in this paper. It is entirely possible that agents are deliberately choosing a number slightly larger or smaller in order to beat the rest to be closer to two-thirds of the average; or that agents are unconsciously selecting a number that is slightly larger or smaller, which end up winning the game. These settings with LLM-based agents most likely demonstrate",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 26,
      "parent_id": "40c5493a789865f39c63b3e583acee11",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "5f9c4ab08cac7457e9111a30e4664920",
    "text": "26",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 26,
      "parent_id": "40c5493a789865f39c63b3e583acee11",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "52841b4997c281b727f7e12f478acec5",
    "text": "the second. Since the information is feed to all agents at the beginning of each period, there are no distinction in what is being observed, therefore the difference lays in that some agents are able to process longer string of information, which technically boils down to token constraints. However, this constraint can potentially distinguish the processing capability or relating to human subjects, the amount of attention to the information given. In which case, having better attention would imply incorporating longer string of information in decision-making. Adopting this interpretation, having better attention could render higher payoffs in certain set-ups though the differences of number chosen between homogeneous agents are almost negligible. Nonetheless, there can also be instances where having more decimal place is detrimental to the outcome, such that the choice is further away from two-thirds of the average than rounded-off numbers selected by players who do not pay as much attention. Intriguingly, this potentially opens up the study of attention in beauty contest game outcomes, which has yet to be addressed.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 27,
      "parent_id": "40c5493a789865f39c63b3e583acee11",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Header",
    "element_id": "e14ee623beef11bc43814a1743a61cb8",
    "text": "27",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 27,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "231887c1dc1c1430258fd9f41682ab41",
    "text": "5 Future Updates and Extensions",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "e14ee623beef11bc43814a1743a61cb8",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "f78a2bfe66b564f2ce9c5c26ec2312da",
    "text": "Much like experiments with human subjects, LLM-based agents could also be sensitive to variations in game design, feedback, as well as instructions provided to them at the beginning of the games.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "231887c1dc1c1430258fd9f41682ab41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "abc4d3b6f6c9c94846cd943c4a970824",
    "text": "Variations in Game Design. In the original beauty contest game proposed by Nagel (1995), which is later called the p-beauty contest game as p can be varied. In this work, I focused on p = 2 3 , such that there exists a unique interior solution, similar set-ups can be done for p = 1 2 and p = 4 3 , which was conducted for human subjects. This paper simply illustrates a case for interactions between LLM-based agents involving strategic complementarity (i.e. p > 0), where there is strategic mimicry among agents. It is possible to evaluate the same set-ups for p < 0, which involves strategic substitution. For the application of results, I can alternatively examine the Cournot market, where agents have to do the opposite of one another in choosing quantities. There are a lot of possibilities in this aspect that merit further exploration in the future.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "231887c1dc1c1430258fd9f41682ab41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "83bef3e4c3052a9706a1177b591a1a4d",
    "text": "Nonetheless, previous experimental design often involve same group of subjects playing for several rounds, random pairing of subjects with different historical information might have implications for extrapolation of out-of-context experiences. While it can be hard to control the information possession of human subjects practically, such variation in game design could be easily simulated with LLM-based agents and would be useful to investigate the potential game play when agents are endowed with individual-specific past game information.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "231887c1dc1c1430258fd9f41682ab41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "fb3e978460e328669b48ccf8ddd00e44",
    "text": "Variations in Feedback Information. Given the line of reasoning discussed in Section 4. It is likely that LLM-based agents are responding to past periods’ average, two-thirds of the average and the winner’s strategy. In this work, past information is fully disclosed. As an extension, it is possible to do the same set-up but with partial feedback to explore the variation in agents’ behaviour. One potential variation of prompt is detailed in A.2. Agents could be given only past rounds’ averages and two-thirds of those averages. This potentially eliminates the possibility of learning from imitation, as well as learning based on gradient ascent learning or experience-weighted attraction, which require revelation of all players’ choices and payoffs. In other words, current period’s choices are expected to be best responses to only the past period average. The proportion of different types of agents would not be formulated as part of the decision problem, and the next period choice is therefore expected to be 0.67 of the past average guess. If the learning pattern does not vary much compared to the current settings, then it could be possible that learning via adjusting the reference points precedes the influence from any other forms of learning.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "231887c1dc1c1430258fd9f41682ab41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "02995b778db70fb3934618a2ce794ee2",
    "text": "Further, it is possible that algorithms might react differently to historical information as compared to humans, thus it could be interesting to understand how LLMs learn from partial feedback as compared to full feedback.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "231887c1dc1c1430258fd9f41682ab41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "647aa0b04639b51b8a555722a20c043e",
    "text": "Objectives. Another aspect that worthy of further exploration would be in the area of prompt engineering. Human sensitivity to problem framing and phrasing of survey questions have long been explored. (Tversky and Kahneman (1981), Kalton and Schuman (1982)) The same applies for LLMs, their decisions are likely to be influenced by the formatting of prompts. (Sclar et al. (2023))",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "231887c1dc1c1430258fd9f41682ab41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "fdc017abd787ab98eadd41fbfdef7242",
    "text": "At the current juncture, the beauty contest games with LLM-based agents mainly aim to understand how would agents behave given the goal of winning the games and then maximizing their payoffs, in most economic models, the focus is usually on maximizing utilities instead of winning. A potential variation that could be interesting to explore would be changing the sequencing of objectives in the last line of instruction, such that the primary objective would be to maximize one’s payoffs, and the secondary goal is to win the games. (Appendix A.2) In this competitive game, the winning strategy is also the strategy that gives the best payoff for each agent, it is therefore unlikely that the decisions given the variation in objective sequencing would result in drastic differences in game outcomes. However, a future update of this work could be to evaluate the same set-ups while varying the prompt slightly, which can serve as sanity check on how much variation in behaviour we would expect the LLM-based agents to have.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "231887c1dc1c1430258fd9f41682ab41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "35391075998f403a25367c1e568072e6",
    "text": "Prompt Language. In Guo et al. (2024), the prompt language was changed to Mandarin Chinese in the multi-LLM-based agents setting. It was found that PaLM is unable to complete the games, indicating potential difficulty in comprehending the instructions when they are given in another language. As for GPT3.5 , it can complete the game in Chinese setting but the choices are more clustered. The variance in strategies observed in this context as compared to the English setting may reflect the differences in strategic behaviours among different language users that the models are trained on, or it could stem from a significantly smaller availability of human-generated data in another language, which is an area that can be improved on to better represent the population. Current work focused on English setting, but future work could involve replicating the set-ups in other prompt languages.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "231887c1dc1c1430258fd9f41682ab41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "2b9386613450bcff8fc8eaa5b4678ec0",
    "text": "Human vs. Computer Interactions. While this work mainly explores LLMs as simulated agents and provides insights on how human subjects might behave given variations in group composition, it also shine a light how we might expect machine vs. machine interactions to be like. Another straightforward and practical extension would be to investigate human vs. machine interactions. Previously, experimental designs involving computers often comprises of pre-defined algorithms. In Coricelli and Nagel (2009), for the treatment group with computer opponent, the computer player is specified to choose uniform randomly 9 numbers within the range of 0 to 100. The authors found that playing against human opponents activated areas in brain that suggest greater strategic reasoning about opponents’ strategies and behaviours, which is not the case when playing with computer that is pre-programmed. This is similar in flavour to the case of simulated subjects playing against fixed-strategy opponents, or",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "parent_id": "231887c1dc1c1430258fd9f41682ab41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Footer",
    "element_id": "2a91a899e08bc3a87d269fa61e19d8c0",
    "text": "28",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 28,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "44b63bf28cb2102c993a3f146ea471ab",
    "text": "static algorithms, in this paper. There is a larger degree of complexity when dealing with LLM-based agents, who could respond dynamically and switch their strategies given historical information. Not only it is intriguing to investigate how human subjects could react to dynamically responsive algorithms, since LLMs do display some degree of learning abilities, it is entirely possible that they are also learning from playing with human subjects, thus observing changes in their performance would also be of interest. This also leads to the question of the potential implications of a feedback loop, where human subjects could attempt to influence the algorithms that in turn affect the other players. There remain lots of research potential in this area.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 29,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Header",
    "element_id": "f0b633a2a1eb8eaa3ede5375606a2c3a",
    "text": "29",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 29,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "444b7d3480001e2e422559c09fc04e41",
    "text": "6 Conclusion",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 30,
      "parent_id": "f0b633a2a1eb8eaa3ede5375606a2c3a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "bd07caec0bda10bf1560e2fda318576a",
    "text": "The contribution of this work is threefold. Firstly, it serves as part of the literature that seeks to make a case for integration of LLMs as tools for social sciences research. It can be utilized to simulate behaviours following known behavioural constraints to see if they fit theoretical predictions, or it can be used to mirror human-like behaviour in a specified environment, such that it resembles experiments with human participants, only that they are ran with simulated agents that can be controlled to some extent. This work focuses on the second scenario, where LLMs are first evaluated using methods conventionally used for human subjects, and the results are related to the types of human subjects that they could represent in terms of strategic levels. Following which, simulated agents are put into certain game settings for us to observe their behaviours.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 30,
      "parent_id": "444b7d3480001e2e422559c09fc04e41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "a92e2ea0d935003e884ff22175d431c4",
    "text": "literature mainly focused on cooperative The second aspect of this paper specified this game setting. While current or anti-coordination games between agents back-boned by a single type of LLM, I explore competitive games between multi-LLM-based agents, and specifically, I explore beauty contest games. Based on the one-shot beauty contests, I characterized the models’ strategic levels based on methods proposed in Nagel (1995), I found LLM-based agents to have strategic levels that fall within the range of 0 to 1 when the reference point is fixed to be mid-point of the range of the numbers they can choose from. In the repeated setting, where multi-LLM-based agents play against each other, they exhibit different learning rates, in terms of their speed of convergence towards the Nash Equilibrium choice. LLM-based agents with relatively lower strategic levels might not show convergence to 0 at the end of 6 periods. The learning behaviour demonstrate potential revision in agents’ belief about their opponents, or them increasing their depth of reasoning, to break it down, when computing the strategic level anchoring to new reference points across periods, I found strategic levels do evolve over time, but the changes are minimal, which in fact complies with results from experiments conducted with human subjects.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 30,
      "parent_id": "444b7d3480001e2e422559c09fc04e41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "bd468a42848b9eb0e0249c5c9b056ddc",
    "text": "To investigate the strategic interactions between LLM-based agents further, I introduce some variations into the games. I select two types of LLM-based agents (high types and low types) based on their strategic levels revealed in the one-shot games, and simulate their behaviours. The interaction between LLM-based agents and fixed-strategy opponents is indicative of potential behaviour when human subjects are playing against static algorithms, usually associated with traditional computerized opponents. In such cases, higher intelligence agents show step-by-step convergence towards 0, while lower intelligence agents could either converge straightaway or stick to a choice determined in period 1 and do not budge. As for the interactions among LLM-based agents, the results suggest plausible outcomes when human subjects are going against each other. I found higher intelligence agents tend to reach 0 more rapidly than lower intelligence agents, and it appears that adjustments in strategies do not occur when the less intelligent agents are playing against one another. When exploring the effects of varying the proportion of agent types in each group, higher intelligence agents converge slower to 0 as the proportion of fixed-strategy agents decreases, demonstrating the impact of increased strategic uncertainty; the same applies for lower intelligence agents - convergence rate declines when there is lower proportion of fixed-strategy opponents - but this is also coupled with greater variations in choices and possibility of no convergence. Given dynamically responsive opponents, agents were found to learn faster when placed in mixed environment with players of different strategic levels than environments that comprises sole of a single pure type. This postulates the potential for application in stimulating faster learning, particularly among less intelligent agents, by introducing heterogeneity into the groups.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 30,
      "parent_id": "444b7d3480001e2e422559c09fc04e41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "90cafd2cc9be113481b73cd62afed720",
    "text": "Last but not least, this work also provides some insights into how different algorithms would behave when interacting with one another. Algorithms could act as proxies for humans or they could become products to be purchased by individuals and firms, and apply to competitive situations. Simulation of strategic interactions between dynamically responsive algorithms and the conventional, static computer algorithms, as well as among the dynamically responsive algorithms themselves, could be useful to learn more about machine behaviour, employing methods used to evaluate humans that can be understood and supported by cognitive theories. In the set-ups illustrated in the paper, even though LLMs may not be able to beat the static algorithm that is pre-defined to play an optimal strategy, they could do comparably well, or in cases of high strategic uncertainty, they could obtain higher payoffs than the static algorithms. As for LLMs vs. LLMs, stronger LLMs, in terms of strategic level, could gain higher payoff when placed in a highly mixed intelligence environment, while weaker LLMs might get a one-shot head-start, but the gain is unsustainable, they would do better in the longer run when there are larger proportion of weaker LLMs in the group. The results show that a mixed environment could improve performance for both LLM types, illustrated by faster speed of convergence, and also surprisingly, having injection of weaker LLMs into game play is beneficial for improving the stronger LLMs, making a case for the usefulness of weaker algorithms.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 30,
      "parent_id": "444b7d3480001e2e422559c09fc04e41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "0d5c6fcb36c38220aae5c2bd4ed18232",
    "text": "I have also attempted at exploring the line of reasoning behind LLM-based agents’ choices and found that they seem to comprehend the game pretty accurately, and they are mainly adjusting their strategies based on the level-k reasoning process of shifting reference points, as well as through imitation of the winners’ strategy. Since LLMs are trained via human generated data that incorporate reasoning processes, the results could give us a hint at the possible logic individuals could have adopted in certain strategic situations.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 30,
      "parent_id": "444b7d3480001e2e422559c09fc04e41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "22a0c4506c078b64b4894bef1fa9a4ec",
    "text": "Since this is a relatively new subject area, there are many possible extensions and great potentials for LLMs to be employed as toolkits for economics research in helping us to interpret and decipher human behaviour. The reverse is true as well, theories and experimental results from decades of learning about human decision-making can be similarly used to better understand machine behaviours and improve their performance to serve specific needs.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 30,
      "parent_id": "444b7d3480001e2e422559c09fc04e41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "624b60c58c9d8bfb6ff1886c2fd605d2",
    "text": "30",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 30,
      "parent_id": "444b7d3480001e2e422559c09fc04e41",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "ebc09b76986ee9dc3b90e79f09f4f7b0",
    "text": "A Appendix",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "f0b633a2a1eb8eaa3ede5375606a2c3a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "e7c82105f09e1bbe80ff54890808cbae",
    "text": "A.1 Original Exemplary Prompt",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "f0b633a2a1eb8eaa3ede5375606a2c3a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "dd95028e976235f9f3265e8d26c96899",
    "text": "One-shot games with multi-LLM-based agents.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "f0b633a2a1eb8eaa3ede5375606a2c3a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "b8e5db2035ed4a3a60d4ebfa2aac6830",
    "text": "[System Instruction.] I want you to act as a clever game player and perform as rationally as possible.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "e877d7e18fff98c5749378a9518c1f72",
    "text": "[User Instruction.] You are playing a game and the rules are as follows:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "089ab412ad30ed7fc01a7d0d59af9bc9",
    "text": "• it consists of {number of players} players, including you.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "e50e8198bd931d2c3f4ce06dd639c5db",
    "text": "• this is a one-round game.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "92259185c7ff9bd300e92cd114606bb7",
    "text": "• you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "a6118f789a2018b49cef5c6980169749",
    "text": "• everyone does not know how others would make choices beforehand.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "2c43b8b301652e7ee91fe7d496b240d3",
    "text": "• the winner is the person whose number is the closest to the two thirds of the average of all chosen numbers.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "a3f779161d20c73be7b3588d74bde314",
    "text": "• the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "b7d9b58cf375e80306d2e52a0904e951",
    "text": "• your goal is primarily to maximize the possibility of getting the prize and secondly to maximize the your prize.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "fe5c9d60d08c6d6b756003d06d4002c4",
    "text": "(Return to 3.1.1)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "dd95028e976235f9f3265e8d26c96899",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "8469e465fc9bb14fe87bf0c9d76fa594",
    "text": "Subseqent Prompt for Historical Information.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "f0b633a2a1eb8eaa3ede5375606a2c3a",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "2579c85bb07d6305930d4c4d7cc10355",
    "text": "[User Instruction.]",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "8469e465fc9bb14fe87bf0c9d76fa594",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "9a1151023996a7fd3b5936c10f748bb8",
    "text": "• The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the agent}: {historical information including (1) period index, (2) choices made by all agents, (3) average of the choices; (4) 2/3 of the average; (5) winner id.}",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "8469e465fc9bb14fe87bf0c9d76fa594",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "b812e2b9ab1b3a6771a0ddde252d473d",
    "text": "• Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "8469e465fc9bb14fe87bf0c9d76fa594",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "4a28ea65cae175cdac12d1908d04ffb9",
    "text": "(Return to 3.1.2) (Return to 3.2.2)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "parent_id": "8469e465fc9bb14fe87bf0c9d76fa594",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Header",
    "element_id": "eec1bf791cc0d5035d4886294f174cd6",
    "text": "31",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 31,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "c16b1a6999c3f42ac78b8134aa1af722",
    "text": "A.2 New Exemplary Prompt",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "eec1bf791cc0d5035d4886294f174cd6",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "37a7ab1274906bd14196f7d26e214e7b",
    "text": "Opponents Playing Fixed Strategy of 0",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "eec1bf791cc0d5035d4886294f174cd6",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "b8e5db2035ed4a3a60d4ebfa2aac6830",
    "text": "[System Instruction.] I want you to act as a clever game player and perform as rationally as possible.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "e877d7e18fff98c5749378a9518c1f72",
    "text": "[User Instruction.] You are playing a game and the rules are as follows:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "089ab412ad30ed7fc01a7d0d59af9bc9",
    "text": "• it consists of {number of players} players, including you.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "e50e8198bd931d2c3f4ce06dd639c5db",
    "text": "• this is a one-round game.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "92259185c7ff9bd300e92cd114606bb7",
    "text": "• you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "a6118f789a2018b49cef5c6980169749",
    "text": "• everyone does not know how others would make choices beforehand.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "2c43b8b301652e7ee91fe7d496b240d3",
    "text": "• the winner is the person whose number is the closest to the two thirds of the average of all chosen numbers.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "a3f779161d20c73be7b3588d74bde314",
    "text": "• the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "b7d9b58cf375e80306d2e52a0904e951",
    "text": "• your goal is primarily to maximize the possibility of getting the prize and secondly to maximize the your prize.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "a4678b01ecc721da2af7caa160f2c36d",
    "text": "• some of your opponents will be playing a fixed strategy of 0 and all others are behaving as rationally as possible.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "5393a6b4bfe28c77029345eb344c8de6",
    "text": "Follow-up for each period. Please just strictly output a JSON string, which has following keys:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "5f55f084c341e4353eb026ce1697e3ac",
    "text": "• understanding: str, your brief understanding of the game",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "fe71e96389d5580ee0bbee3268b69007",
    "text": "• popular answer: float, the number which you think other players are most likely to choose",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "448475d225b05610dacc1ae210f657c8",
    "text": "• answer: float, the number which you would like to choose",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "9f78ddb24834421b5baa7135d4141634",
    "text": "• reason: str, the brief reason why you give the popular answer and the answer that way",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "6617151053362436003b0d49ace6532d",
    "text": "Subsequent Prompt (after period 1).",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "9a1151023996a7fd3b5936c10f748bb8",
    "text": "• The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the agent}: {historical information including (1) period index, (2) choices made by all agents, (3) average of the choices; (4) 2/3 of the average; (5) winner id.}",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "b812e2b9ab1b3a6771a0ddde252d473d",
    "text": "• Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "b1ebb523f2a6fcd5edece7b1209ecf11",
    "text": "(Return to 3.2.1)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "parent_id": "37a7ab1274906bd14196f7d26e214e7b",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Header",
    "element_id": "3cb6bc651b03d994974629eacef96963",
    "text": "32",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 32,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "57df6a078db3acd6a9e996e86a6365c8",
    "text": "Partial Feedback: Subsequent Prompt for Historical Information.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "3cb6bc651b03d994974629eacef96963",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "2579c85bb07d6305930d4c4d7cc10355",
    "text": "[User Instruction.]",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "57df6a078db3acd6a9e996e86a6365c8",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "dd1802faa9f845c758be639c1c34e7e7",
    "text": "• The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the agent}: {historical information including (1) period index, (2) average of the choices; (3) 2/3 of the average.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "57df6a078db3acd6a9e996e86a6365c8",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "b812e2b9ab1b3a6771a0ddde252d473d",
    "text": "• Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "57df6a078db3acd6a9e996e86a6365c8",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "6611810bc0fab61db3869e818bf445a1",
    "text": "(Return to 5)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "57df6a078db3acd6a9e996e86a6365c8",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "cac15c9eac47a42bc4b2f989be3fec40",
    "text": "Changing Objective Sequence: Optimizing before Winning",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "3cb6bc651b03d994974629eacef96963",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "b8e5db2035ed4a3a60d4ebfa2aac6830",
    "text": "[System Instruction.] I want you to act as a clever game player and perform as rationally as possible.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "e877d7e18fff98c5749378a9518c1f72",
    "text": "[User Instruction.] You are playing a game and the rules are as follows:",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "089ab412ad30ed7fc01a7d0d59af9bc9",
    "text": "• it consists of {number of players} players, including you.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "e50e8198bd931d2c3f4ce06dd639c5db",
    "text": "• this is a one-round game.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "92259185c7ff9bd300e92cd114606bb7",
    "text": "• you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "a6118f789a2018b49cef5c6980169749",
    "text": "• everyone does not know how others would make choices beforehand.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "2c43b8b301652e7ee91fe7d496b240d3",
    "text": "• the winner is the person whose number is the closest to the two thirds of the average of all chosen numbers.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "a3f779161d20c73be7b3588d74bde314",
    "text": "• the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "fba363acf347a270bcb7f91a4a496e4b",
    "text": "• your goal is primarily to maximize your prize and secondly to maximize your possibility of getting the prize.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "5b60fbcf93c14c0c7c58530b93ec4f9e",
    "text": "*Follow-up for each period, and subsequent prompt (after period 1). (same as before) (Return to 5)",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "parent_id": "cac15c9eac47a42bc4b2f989be3fec40",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Header",
    "element_id": "b373ad2af43d19b216f0f407c75a2edb",
    "text": "33",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 33,
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Title",
    "element_id": "a0d7deccf89e42d02a9d66b0c1889689",
    "text": "References",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "b373ad2af43d19b216f0f407c75a2edb",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "b6032eea063f91c273bdbae298126f9a",
    "text": "Aher, G. V., Arriaga, R. I., and Kalai, A. T. (2023). Using large language models to simulate multiple humans and replicate human subject studies. In International Conference on Machine Learning, pages 337–371. PMLR.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "ef5beb7a133228700808d1125ff9a76c",
    "text": "Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M., and Schulz, E. (2023). Playing repeated games with large language models. arXiv preprint arXiv:2305.16867.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "600d51aa05bcf45b797538accbbe078d",
    "text": "Argyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R., Rytting, C., and Wingate, D. (2023). Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3):337–351.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "a9b0e0a9ef3a3f4f2bc7ba35d1fda80d",
    "text": "Bosch-Domenech, A., Montalvo, J. G., Nagel, R., and Satorra, A. (2002). One, two,(three), infinity,. . . : Newspaper and lab beauty-contest experiments. American Economic Review, 92(5):1687–1701.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "0430590918a9c1b49fe54620c3f65d7c",
    "text": "Brown, Z. Y. and MacKay, A. (2023). Competition in pricing algorithms. American Economic Journal: Microeconomics, 15(2):109–156.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "d30467eb04d63b6d2eab82f4d511d210",
    "text": "Camerer, C. F., Ho, T.-H., and Chong, J.-K. (2004). A cognitive hierarchy model of games. The Quarterly Journal of Economics, 119(3):861–898.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "6fbbb6c6877ec0dc6282428be4572701",
    "text": "Chen, L., Mislove, A., and Wilson, C. (2016). An empirical analysis of algorithmic pricing on amazon marketplace. In Proceedings of the 25th international conference on World Wide Web, pages 1339–1349.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "0a841a4982c8d1b8085e60ada6648388",
    "text": "Chollet, F. (2021). Deep learning with Python. Simon and Schuster.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "394491be491f0a9631a6512aa62a3ac9",
    "text": "Coricelli, G. and Nagel, R. (2009). Neural correlates of depth of strategic reasoning in medial prefrontal cortex. Proceedings of the National Academy of Sciences, 106(23):9163–9168.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "f0095683e328a5caf7ae8979d418afac",
    "text": "Costa-Gomes, M. A. and Weizs¨acker, G. (2008). Stated beliefs and play in normal-form games. The Review of Economic Studies, 75(3):729–762.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "bd966600d8d149af6e56ba73ed4bd215",
    "text": "Devetag, G., Di Guida, S., and Polonio, L. (2016). An eye-tracking study of feature-based choice in one-shot games. Experimental Economics, 19:177–201.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "88b8a605a5ddb72c42c157d6b6ac636f",
    "text": "Dillion, D., Tandon, N., Gu, Y., and Gray, K. (2023). Can ai language models replace human participants? Trends in Cognitive Sciences.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "bf33f50d6c9e1682b991a77066f451b2",
    "text": "Duffy, J., Hopkins, E., and Kornienko, T. (2021). Facing the grim truth: Repeated prisoner’s dilemma against robot opponents. Technical report, Working Paper.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "297e3b5607f0de75e1832393586713b0",
    "text": "Fan, C., Chen, J., Jin, Y., and He, H. (2023). Can large language models serve as rational players in game theory? a systematic analysis. arXiv preprint arXiv:2312.05488.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "b3f729de5f82e49b8e90ccf598ec3c45",
    "text": "Goldberg, D. (1991). What every computer scientist should know about floating-point arithmetic. ACM computing surveys (CSUR), 23(1):5–48.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "1874fec58556843e7382cf06e53a130f",
    "text": "Guo, F. (2023). Gpt in game theory experiments. arXiv:2305.05516.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "ae7987e63a3ddf0976017f063a876e3a",
    "text": "Guo, S., Bu, H., Wang, H., Ren, Y., Sui, D., Shang, Y., and Lu, S. (2024). Economics arena for large language models. arXiv preprint arXiv:2401.01735.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "6373ce3c178bbc53574157fad33375c8",
    "text": "Horton, J. J. (2023). Large language models as simulated economic agents: What can we learn from homo silicus? Technical report, National Bureau of Economic Research.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "7c8eccf4f0a7489b294f67ca439527b6",
    "text": "Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., et al. (2023). A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "70e5e9bee0795287b135d2d893451fb6",
    "text": "HuggingFace (2022). Illustrating reinforcement learning from human feedback (rlhf).",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "2bd03148394b710c720fa2ae52aff20c",
    "text": "Huijzer, R. and Hill, Y. (2023). Large language models show human behavior.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "f77cbabb5fa8c87cd053acc2c8ad386a",
    "text": "IBM (2024). Tokens and tokenization. Accessed: 2024-04-10.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "5d92076015d78bf44dbcd6741529f001",
    "text": "Ireson, J. and Hallam, S. (1999). Raising standards: Is ability grouping the answer? Oxford review of education, 25(3):343–358.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "e4943853c6f4d242cef9e359f665f4e7",
    "text": "Johnston, O., Wildy, H., and Shand, J. (2023). Students’ contrasting their experiences of teacher expectations in streamed and mixed ability classes: A study of grade 10 students in western australia. Research Papers in Education, 38(4):543–567.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "7ef6fd2164bc01a315582453215fbe02",
    "text": "Joyce, P. and McMillan, B. (2010). Student perceptions of their learning experience in streamed and mixed-ability classes. Language Education in Asia, 1(1):215–227.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "51cdfeff5eac5edb1bbedf9253b6f012",
    "text": "Kalton, G. and Schuman, H. (1982). The effect of the question on survey responses: A review. Journal of the Royal Statistical Society Series A: Statistics in Society, 145(1):42–57.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "652954c582157860c6da2f926ff2904f",
    "text": "Keynes, J. M. (1936). The general theory of interest, employment and money.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "37261ec5bc31e5551f1c84791fd588c1",
    "text": "Kosinski, M. (2023). Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "49af925a7f5c539acdf145ae23a5cc77",
    "text": "Labellerr (2024). Training small-scale vs large-scale language models: The difference.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "354d6a6ad4590e3b6d77d655261be4c7",
    "text": "Liem, G. A. D., Marsh, H. W., Martin, A. J., McInerney, D. M., and Yeung, A. S. (2013). The big-fish-little-pond effect and a national policy of within-school ability streaming: Alternative frames of reference. American Educational Research Journal, 50(2):326–370.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "UncategorizedText",
    "element_id": "86e50149658661312a9e0b35558d84f6",
    "text": "34",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 34,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "aaf74c6b96d42f23c5492b357983f32a",
    "text": "Mauersberger, F. and Nagel, R. (2018). Levels of reasoning in keynesian beauty contests: a generative framework. In Handbook of computational economics, volume 4, pages 541–634. Elsevier.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "02cb78f8f6651a6cb629d6eb971fe801",
    "text": "Mei, Q., Xie, Y., Yuan, W., and Jackson, M. O. (2024). A turing test of whether ai chatbots are behaviorally similar to humans. Proceedings of the National Academy of Sciences, 121(9):e2313925121.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "19948131b66f28440689238248c5facc",
    "text": "Nagel, R. (1995). Unraveling in guessing games: An experimental study. The American economic review, 85(5):1313–1326.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "dde6f7883fa20631165b77a060bd3bf9",
    "text": "Nagel, R., B¨uhren, C., and Frank, B. (2017). Inspired and inspiring: Herv´e moulin and the discovery of the beauty contest game. Mathematical Social Sciences, 90:191–207.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "f549a8da1dea039f6573349ecac28402",
    "text": "OpenAI (2024). How chatgpt and our language models are developed.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "ddeda4dbf881368afb1e2fa237ecb994",
    "text": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "120323821cd76543289bf509de5cb194",
    "text": "Phelps, S. and Russell, Y. I. (2023). Investigating emergent goal-like behaviour in large language models using experimental economics. arXiv preprint arXiv:2305.07970.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "2d0716a32c527ddddcb3c41f03bbb069",
    "text": "Proto, E., Rustichini, A., and Sofianos, A. (2022). Intelligence, errors, and cooperation in repeated interactions. The Review of Economic Studies, 89(5):2723–2767.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "3a59f2694f749d755e80ea0e6e246295",
    "text": "Pulverm¨uller, F., Tomasello, R., Henningsen-Schomers, M. R., and Wennekers, T. (2021). Biological constraints on neural network models of cognitive function. Nature Reviews Neuroscience, 22(8):488–502.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "1250bd4eba916d6dcd72808409de5bd8",
    "text": "Sclar, M., Choi, Y., Tsvetkov, Y., and Suhr, A. (2023). Quantifying language models’ sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "c0a1c6949b17b41df2780ee5fef42b40",
    "text": "Trality (2024). Crypto trading bots: The ultimate beginner’s guide.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "NarrativeText",
    "element_id": "a0cb899a03af23f5ad0bf0dc6f83206f",
    "text": "Tversky, A. and Kahneman, D. (1981). The framing of decisions and the psychology of choice. science, 211(4481):453–458.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "ListItem",
    "element_id": "f935b5aaf2c66d5b5524870255d17aae",
    "text": "Webb, T., Holyoak, K. J., and Lu, H. (2023). Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):1526–1541.",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "parent_id": "a0d7deccf89e42d02a9d66b0c1889689",
      "filename": "2404.08492.pdf"
    }
  },
  {
    "type": "Header",
    "element_id": "d7ec99bf67f290d4bbd0b382f4a986e0",
    "text": "35",
    "metadata": {
      "filetype": "application/pdf",
      "languages": [
        "eng"
      ],
      "page_number": 35,
      "filename": "2404.08492.pdf"
    }
  }
]
